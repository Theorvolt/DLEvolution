{
    "html_url": {
        "0": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.10.0-alpha.3",
        "1": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.3",
        "2": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.2",
        "3": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.1",
        "4": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.12",
        "5": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.11",
        "6": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0",
        "7": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.10",
        "8": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.9",
        "9": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.8",
        "10": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.2",
        "11": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.7",
        "12": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.6",
        "13": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.1",
        "14": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.5",
        "15": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.4",
        "16": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0",
        "17": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.2",
        "18": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.0",
        "19": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-prod-ctcdecode",
        "20": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-prod",
        "21": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.9",
        "22": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.8",
        "23": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.7",
        "24": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.6",
        "25": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.5",
        "26": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.4",
        "27": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.3",
        "28": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.2",
        "29": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.1",
        "30": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.0.1-alpha",
        "31": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/0.2.0-alpha.0",
        "32": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.8",
        "33": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.7",
        "34": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.3",
        "35": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.2",
        "36": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.1",
        "37": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.6",
        "38": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.5",
        "39": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.9.0-alpha.0",
        "40": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.4",
        "41": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.4",
        "42": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.3",
        "43": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.3",
        "44": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.2",
        "45": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.1",
        "46": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.8.0-alpha.0",
        "47": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.1",
        "48": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.1-alpha.2",
        "49": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.1-alpha.1",
        "50": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.1-alpha.0",
        "51": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.0-alpha.4",
        "52": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.0",
        "53": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.0-alpha.3",
        "54": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.0-alpha.2",
        "55": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.0-alpha.1",
        "56": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.7.0-alpha.0",
        "57": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.1-alpha.0",
        "58": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.1",
        "59": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.15",
        "60": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.14",
        "61": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.13",
        "62": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.12",
        "63": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.11",
        "64": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.10",
        "65": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.9",
        "66": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0",
        "67": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.8",
        "68": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.7",
        "69": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.6",
        "70": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.5",
        "71": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.4",
        "72": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.3",
        "73": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.2",
        "74": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.6.0-alpha.1",
        "75": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.1",
        "76": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.11",
        "77": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0",
        "78": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.10",
        "79": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.9",
        "80": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.8",
        "81": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.7",
        "82": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.6",
        "83": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.5",
        "84": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.4",
        "85": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.3",
        "86": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.2",
        "87": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.1",
        "88": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.5.0-alpha.0",
        "89": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.4.1",
        "90": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.4.0-alpha.3",
        "91": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.4.0-alpha.2",
        "92": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.4.0-alpha.1",
        "93": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.4.0",
        "94": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.4.0-alpha.0",
        "95": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.3.0-alpha.1",
        "96": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.3.0-alpha.0",
        "97": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.3.0",
        "98": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.1-alpha.2",
        "99": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.1-alpha.1",
        "100": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.1-alpha.0",
        "101": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0-alpha.10",
        "102": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.2.0",
        "103": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.1.1",
        "104": "https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/tag\/v0.1.0"
    },
    "tag_name": {
        "0": "v0.10.0-alpha.3",
        "1": "v0.9.3",
        "2": "v0.9.2",
        "3": "v0.9.1",
        "4": "v0.9.0-alpha.12",
        "5": "v0.9.0-alpha.11",
        "6": "v0.9.0",
        "7": "v0.9.0-alpha.10",
        "8": "v0.9.0-alpha.9",
        "9": "v0.9.0-alpha.8",
        "10": "v0.8.2",
        "11": "v0.9.0-alpha.7",
        "12": "v0.9.0-alpha.6",
        "13": "v0.8.1",
        "14": "v0.9.0-alpha.5",
        "15": "v0.9.0-alpha.4",
        "16": "v0.8.0",
        "17": "v0.7.2",
        "18": "v0.6.0-alpha.0",
        "19": "v0.2.0-prod-ctcdecode",
        "20": "v0.2.0-prod",
        "21": "v0.2.0-alpha.9",
        "22": "v0.2.0-alpha.8",
        "23": "v0.2.0-alpha.7",
        "24": "v0.2.0-alpha.6",
        "25": "v0.2.0-alpha.5",
        "26": "v0.2.0-alpha.4",
        "27": "v0.2.0-alpha.3",
        "28": "v0.2.0-alpha.2",
        "29": "v0.2.0-alpha.1",
        "30": "v0.0.1-alpha",
        "31": "0.2.0-alpha.0",
        "32": "v0.8.0-alpha.8",
        "33": "v0.8.0-alpha.7",
        "34": "v0.9.0-alpha.3",
        "35": "v0.9.0-alpha.2",
        "36": "v0.9.0-alpha.1",
        "37": "v0.8.0-alpha.6",
        "38": "v0.8.0-alpha.5",
        "39": "v0.9.0-alpha.0",
        "40": "v0.8.0-alpha.4",
        "41": "v0.7.4",
        "42": "v0.8.0-alpha.3",
        "43": "v0.7.3",
        "44": "v0.8.0-alpha.2",
        "45": "v0.8.0-alpha.1",
        "46": "v0.8.0-alpha.0",
        "47": "v0.7.1",
        "48": "v0.7.1-alpha.2",
        "49": "v0.7.1-alpha.1",
        "50": "v0.7.1-alpha.0",
        "51": "v0.7.0-alpha.4",
        "52": "v0.7.0",
        "53": "v0.7.0-alpha.3",
        "54": "v0.7.0-alpha.2",
        "55": "v0.7.0-alpha.1",
        "56": "v0.7.0-alpha.0",
        "57": "v0.6.1-alpha.0",
        "58": "v0.6.1",
        "59": "v0.6.0-alpha.15",
        "60": "v0.6.0-alpha.14",
        "61": "v0.6.0-alpha.13",
        "62": "v0.6.0-alpha.12",
        "63": "v0.6.0-alpha.11",
        "64": "v0.6.0-alpha.10",
        "65": "v0.6.0-alpha.9",
        "66": "v0.6.0",
        "67": "v0.6.0-alpha.8",
        "68": "v0.6.0-alpha.7",
        "69": "v0.6.0-alpha.6",
        "70": "v0.6.0-alpha.5",
        "71": "v0.6.0-alpha.4",
        "72": "v0.6.0-alpha.3",
        "73": "v0.6.0-alpha.2",
        "74": "v0.6.0-alpha.1",
        "75": "v0.5.1",
        "76": "v0.5.0-alpha.11",
        "77": "v0.5.0",
        "78": "v0.5.0-alpha.10",
        "79": "v0.5.0-alpha.9",
        "80": "v0.5.0-alpha.8",
        "81": "v0.5.0-alpha.7",
        "82": "v0.5.0-alpha.6",
        "83": "v0.5.0-alpha.5",
        "84": "v0.5.0-alpha.4",
        "85": "v0.5.0-alpha.3",
        "86": "v0.5.0-alpha.2",
        "87": "v0.5.0-alpha.1",
        "88": "v0.5.0-alpha.0",
        "89": "v0.4.1",
        "90": "v0.4.0-alpha.3",
        "91": "v0.4.0-alpha.2",
        "92": "v0.4.0-alpha.1",
        "93": "v0.4.0",
        "94": "v0.4.0-alpha.0",
        "95": "v0.3.0-alpha.1",
        "96": "v0.3.0-alpha.0",
        "97": "v0.3.0",
        "98": "v0.2.1-alpha.2",
        "99": "v0.2.1-alpha.1",
        "100": "v0.2.1-alpha.0",
        "101": "v0.2.0-alpha.10",
        "102": "v0.2.0",
        "103": "v0.1.1",
        "104": "v0.1.0"
    },
    "name": {
        "0": "v0.10.0-alpha.3",
        "1": "DeepSpeech 0.9.3",
        "2": "DeepSpeech 0.9.2",
        "3": "DeepSpech 0.9.1",
        "4": "v0.9.0-alpha.12",
        "5": "v0.9.0-alpha.11",
        "6": "DeepSpech 0.9.0",
        "7": "v0.9.0-alpha.10",
        "8": "v0.9.0-alpha.9",
        "9": "v0.9.0-alpha.8",
        "10": "DeepSpeech 0.8.2",
        "11": "v0.9.0-alpha.7",
        "12": "v0.9.0-alpha.6",
        "13": "DeepSpeech 0.8.1",
        "14": "v0.9.0-alpha.5",
        "15": "v0.9.0-alpha.4",
        "16": "DeepSpeech 0.8.0",
        "17": "v0.7.2",
        "18": "v0.6.0-alpha.0",
        "19": "v0.2.0-prod-ctcdecode",
        "20": "v0.2.0-prod",
        "21": "v0.2.0-alpha.9",
        "22": "v0.2.0-alpha.8",
        "23": "v0.2.0-alpha.7",
        "24": "v0.2.0-alpha.6",
        "25": "v0.2.0-alpha.5",
        "26": "v0.2.0-alpha.4",
        "27": "v0.2.0-alpha.3",
        "28": "v0.2.0-alpha.2",
        "29": "v0.2.0-alpha.1",
        "30": "v0.0.1-alpha",
        "31": "0.2.0-alpha.0",
        "32": "v0.8.0-alpha.8",
        "33": "v0.8.0-alpha.7",
        "34": "v0.9.0-alpha.3",
        "35": "v0.9.0-alpha.2",
        "36": "v0.9.0-alpha.1",
        "37": "v0.8.0-alpha.6",
        "38": "v0.8.0-alpha.5",
        "39": "v0.9.0-alpha.0",
        "40": "v0.8.0-alpha.4",
        "41": "DeepSpeech 0.7.4",
        "42": "v0.8.0-alpha.3",
        "43": "DeepSpeech 0.7.3",
        "44": "v0.8.0-alpha.2",
        "45": "v0.8.0-alpha.1",
        "46": "v0.8.0-alpha.0",
        "47": "DeepSpeech 0.7.1",
        "48": "v0.7.1-alpha.2",
        "49": "v0.7.1-alpha.1",
        "50": "v0.7.1-alpha.0",
        "51": "v0.7.0-alpha.4",
        "52": "DeepSpeech 0.7.0",
        "53": "v0.7.0-alpha.3",
        "54": "v0.7.0-alpha.2",
        "55": "v0.7.0-alpha.1",
        "56": "v0.7.0-alpha.0",
        "57": "v0.6.1-alpha.0",
        "58": "DeepSpeech 0.6.1",
        "59": "v0.6.0-alpha.15",
        "60": "v0.6.0-alpha.14",
        "61": "v0.6.0-alpha.13",
        "62": "v0.6.0-alpha.12",
        "63": "v0.6.0-alpha.11",
        "64": "v0.6.0-alpha.10",
        "65": "v0.6.0-alpha.9",
        "66": "DeepSpeech 0.6.0",
        "67": "v0.6.0-alpha.8",
        "68": "v0.6.0-alpha.7",
        "69": "v0.6.0-alpha.6",
        "70": "v0.6.0-alpha.5",
        "71": "v0.6.0-alpha.4",
        "72": "v0.6.0-alpha.3",
        "73": "v0.6.0-alpha.2",
        "74": "v0.6.0-alpha.1",
        "75": "DeepSpeech 0.5.1",
        "76": "v0.5.0-alpha.11",
        "77": "Deep Speech 0.5.0",
        "78": "v0.5.0-alpha.10",
        "79": "v0.5.0-alpha.9",
        "80": "v0.5.0-alpha.8",
        "81": "v0.5.0-alpha.7",
        "82": "v0.5.0-alpha.6",
        "83": "v0.5.0-alpha.5",
        "84": "v0.5.0-alpha.4",
        "85": "v0.5.0-alpha.3",
        "86": "v0.5.0-alpha.2",
        "87": "v0.5.0-alpha.1",
        "88": "v0.5.0-alpha.0",
        "89": "Deep Speech 0.4.1",
        "90": "v0.4.0-alpha.3",
        "91": "v0.4.0-alpha.2",
        "92": "v0.4.0-alpha.1",
        "93": "Deep Speech 0.4.0",
        "94": "v0.4.0-alpha.0",
        "95": "v0.3.0-alpha.1",
        "96": "v0.3.0-alpha.0",
        "97": "Deep Speech 0.3.0",
        "98": "v0.2.1-alpha.2",
        "99": "v0.2.1-alpha.1",
        "100": "v0.2.1-alpha.0",
        "101": "v0.2.0-alpha.10",
        "102": "Deep Speech 0.2.0",
        "103": "Deep Speech 0.1.1",
        "104": "Deep Speech 0.1.0"
    },
    "prerelease": {
        "0": true,
        "1": false,
        "2": false,
        "3": false,
        "4": true,
        "5": true,
        "6": false,
        "7": true,
        "8": true,
        "9": true,
        "10": false,
        "11": true,
        "12": true,
        "13": false,
        "14": true,
        "15": true,
        "16": false,
        "17": false,
        "18": true,
        "19": true,
        "20": true,
        "21": true,
        "22": true,
        "23": true,
        "24": true,
        "25": true,
        "26": true,
        "27": true,
        "28": true,
        "29": true,
        "30": true,
        "31": true,
        "32": true,
        "33": true,
        "34": true,
        "35": true,
        "36": true,
        "37": true,
        "38": true,
        "39": true,
        "40": true,
        "41": false,
        "42": true,
        "43": false,
        "44": true,
        "45": true,
        "46": true,
        "47": false,
        "48": true,
        "49": true,
        "50": true,
        "51": true,
        "52": false,
        "53": true,
        "54": true,
        "55": true,
        "56": true,
        "57": true,
        "58": false,
        "59": true,
        "60": true,
        "61": true,
        "62": true,
        "63": true,
        "64": true,
        "65": true,
        "66": false,
        "67": true,
        "68": true,
        "69": true,
        "70": true,
        "71": true,
        "72": true,
        "73": true,
        "74": true,
        "75": false,
        "76": true,
        "77": false,
        "78": true,
        "79": true,
        "80": true,
        "81": true,
        "82": true,
        "83": true,
        "84": true,
        "85": true,
        "86": true,
        "87": true,
        "88": true,
        "89": false,
        "90": true,
        "91": true,
        "92": true,
        "93": false,
        "94": true,
        "95": true,
        "96": true,
        "97": false,
        "98": true,
        "99": true,
        "100": true,
        "101": true,
        "102": false,
        "103": false,
        "104": false
    },
    "published_at": {
        "0": "2020-12-19T10:13:23Z",
        "1": "2020-12-10T15:58:47Z",
        "2": "2020-12-03T16:40:18Z",
        "3": "2020-11-04T16:56:50Z",
        "4": "2020-10-30T17:17:02Z",
        "5": "2020-10-09T13:26:13Z",
        "6": "2020-11-02T13:07:03Z",
        "7": "2020-09-25T12:29:34Z",
        "8": "2020-09-21T11:30:13Z",
        "9": "2020-09-10T08:14:34Z",
        "10": "2020-08-22T14:38:12Z",
        "11": "2020-08-18T14:09:37Z",
        "12": "2020-08-12T18:15:01Z",
        "13": "2020-08-11T08:25:48Z",
        "14": "2020-08-07T12:44:58Z",
        "15": "2020-08-06T13:25:28Z",
        "16": "2020-07-30T17:16:00Z",
        "17": "2020-07-28T17:56:34Z",
        "18": "2020-07-28T15:54:29Z",
        "19": "2020-07-28T13:49:02Z",
        "20": "2020-07-28T13:48:45Z",
        "21": "2020-07-28T13:46:01Z",
        "22": "2020-07-28T13:43:14Z",
        "23": "2020-07-28T13:42:58Z",
        "24": "2020-07-28T13:42:44Z",
        "25": "2020-07-28T13:39:52Z",
        "26": "2020-07-28T13:37:07Z",
        "27": "2020-07-28T13:34:17Z",
        "28": "2020-07-28T13:30:43Z",
        "29": "2020-07-28T13:23:16Z",
        "30": "2020-07-28T13:13:58Z",
        "31": "2020-07-28T13:09:03Z",
        "32": "2020-07-27T14:30:11Z",
        "33": "2020-07-15T22:18:13Z",
        "34": "2020-07-15T20:33:40Z",
        "35": "2020-07-07T10:12:24Z",
        "36": "2020-07-06T10:25:50Z",
        "37": "2020-07-04T14:08:38Z",
        "38": "2020-07-03T14:18:33Z",
        "39": "2020-06-24T19:12:36Z",
        "40": "2020-06-23T19:45:46Z",
        "41": "2020-06-18T14:57:08Z",
        "42": "2020-06-09T07:04:44Z",
        "43": "2020-06-04T09:29:32Z",
        "44": "2020-05-27T19:16:21Z",
        "45": "2020-05-26T20:30:58Z",
        "46": "2020-05-26T13:03:25Z",
        "47": "2020-05-12T15:31:11Z",
        "48": "2020-05-07T13:09:40Z",
        "49": "2020-05-04T12:05:27Z",
        "50": "2020-05-01T21:17:57Z",
        "51": "2020-04-24T13:10:08Z",
        "52": "2020-04-24T16:17:29Z",
        "53": "2020-03-25T14:27:39Z",
        "54": "2020-02-17T15:46:04Z",
        "55": "2020-02-03T16:13:22Z",
        "56": "2020-01-31T12:30:02Z",
        "57": "2019-12-13T17:09:58Z",
        "58": "2020-01-10T17:12:08Z",
        "59": "2019-11-14T09:51:19Z",
        "60": "2019-11-07T07:55:06Z",
        "61": "2019-11-05T17:27:37Z",
        "62": "2019-11-05T10:17:42Z",
        "63": "2019-10-26T11:26:53Z",
        "64": "2019-10-17T07:55:20Z",
        "65": "2019-10-11T17:32:16Z",
        "66": "2019-12-03T17:18:08Z",
        "67": "2019-09-27T11:15:49Z",
        "68": "2019-09-24T13:43:18Z",
        "69": "2019-09-19T17:19:57Z",
        "70": "2019-08-22T11:25:29Z",
        "71": "2019-07-12T17:53:04Z",
        "72": "2019-07-11T13:43:11Z",
        "73": "2019-07-05T20:57:50Z",
        "74": "2019-06-25T12:28:33Z",
        "75": "2019-06-20T17:27:39Z",
        "76": "2019-05-31T03:19:54Z",
        "77": "2019-06-11T15:25:34Z",
        "78": "2019-05-22T20:13:42Z",
        "79": "2019-05-22T08:28:46Z",
        "80": "2019-05-10T18:31:44Z",
        "81": "2019-04-26T07:32:23Z",
        "82": "2019-04-25T09:18:26Z",
        "83": "2019-04-08T14:57:08Z",
        "84": "2019-03-20T20:26:11Z",
        "85": "2019-03-20T06:49:18Z",
        "86": "2019-03-13T14:00:30Z",
        "87": "2019-01-24T17:34:45Z",
        "88": "2019-01-23T13:46:04Z",
        "89": "2019-01-10T14:47:43Z",
        "90": "2018-12-19T11:34:27Z",
        "91": "2018-12-14T11:54:49Z",
        "92": "2018-12-12T08:46:57Z",
        "93": "2019-01-10T14:49:55Z",
        "94": "2018-11-01T12:26:33Z",
        "95": "2018-10-13T10:26:04Z",
        "96": "2018-10-11T17:30:14Z",
        "97": "2018-10-23T15:58:49Z",
        "98": "2018-10-02T16:49:21Z",
        "99": "2018-09-26T18:08:49Z",
        "100": "2018-09-26T11:35:33Z",
        "101": "2018-09-18T15:01:36Z",
        "102": "2018-09-18T22:14:11Z",
        "103": "2018-09-18T12:15:21Z",
        "104": "2017-11-24T15:39:42Z"
    },
    "body": {
        "0": "",
        "1": "# General\r\n\r\nThis is the 0.9.3 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with earlier versions. However, models exported for 0.7.X and 0.8.X should work with this release. **This is a bugfix release and retains compatibility with the 0.9.0, 0.9.1 and 0.9.2 models. All model files included here are identical to the ones in the 0.9.0 release.** As with previous releases, this release includes the source code:\r\n\r\n[v0.9.3.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.9.3.tar.gz)\r\n\r\nUnder the [MPL-2.0 license](https:\/\/www.mozilla.org\/en-US\/MPL\/2.0\/). And the acoustic models:\r\n\r\n[deepspeech-0.9.3-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-models.pbmm)\r\n[deepspeech-0.9.3-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-models.tflite)\r\n\r\nIn addition we're releasing experimental Mandarin Chinese acoustic models trained on an internal corpus composed of 2000h of read speech:\r\n\r\n[deepspeech-0.9.3-models-zh-CN.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-models-zh-CN.pbmm)\r\n[deepspeech-0.9.3-models-zh-CN.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-models-zh-CN.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model files with the \".pbmm\" extension are memory mapped and thus memory efficient and fast to load. The model files with the \".tflite\" extension are converted to use TensorFlow Lite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and are more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English with synthetic noise augmentation and the .pbmm model achieves an 7.06% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.9.3-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nThere is also a corresponding scorer for the Mandarin Chinese model:\r\n\r\n[deepspeech-0.9.3-models-zh-CN.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-models-zh-CN.scorer)\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.9.3.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/audio-0.9.3.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files for both the English and Mandarin models:\r\n\r\n[deepspeech-0.9.3-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-checkpoint.tar.gz)\r\n[deepspeech-0.9.3-checkpoint-zh-CN.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.3\/deepspeech-0.9.3-checkpoint-zh-CN.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Add CI testing for hot word boosting on .NET bindings ([#3416](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3416))\r\n- Improve error message on generate_scorer_package tooling ([#3435](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3435))\r\n- Enable support for building static iOS framework ([#3436](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3436))\r\n- Change Java binding package name from org.mozilla.deepspeech to org.deepspeech ([#3454](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3454))\r\n- Expose Stream type on TypeScript binding ([#3456](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3456))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred as a fine tuning of the previous 0.8.2 checkpoint, with data augmentation options enabled. The following hyperparameters were used for the fine tuning. See the 0.8.2 release notes for the hyperparameters used for the base model.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 200\r\n  * `augment` `pitch[pitch=1~0.1]`\r\n  * `augment` `tempo[factor=1~0.1]`\r\n  * `augment` `overlay[p=0.9,source=${noise},layers=1,snr=12~4]` (where ${noise} is a dataset of Freesound.org background noise recordings)\r\n  * `augment` `overlay[p=0.1,source=${voices},layers=10~2,snr=12~4]` (where ${voices} is a dataset of audiobook snippets extracted from Librivox)\r\n  * `augment` `resample[p=0.2,rate=12000~4000]`\r\n  * `augment` `codec[p=0.2,bitrate=32000~16000]`\r\n  * `augment` `reverb[p=0.2,decay=0.7~0.15,delay=10~8]`\r\n  * `augment` `volume[p=0.2,dbfs=-10~10]`\r\n  * `cache_for_epochs` 10\r\n\r\nThe weights with the best validation loss were selected at the end of 200 epochs using `--noearly_stop`.\r\n\r\nThe optimal `lm_alpha` and `lm_beta` values with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus remain unchanged from the previous release:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\nFor the Mandarin Chinese model, the following values are recommended:\r\n\r\n  * `lm_alpha` 0.6940122363709647\r\n  * `lm_beta` 4.777924224113021\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.3\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7, 3.8 and 3.9) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.3\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x, 14.x and 15.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, 9.1, 9.2, 10.0, 10.1, and 11.0 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.3\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.3\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.3) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.3).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n* TFLite Delegation API is here as a preview: do not expect released models to work out-of-the box, but feedback \/ PRs is welcome.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.3\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.9.3 release\r\n\r\n- Alexandre Lissy\r\n- Catalin Voss\r\n- Olaf Thiele\r\n- Reuben Morais",
        "2": "# General\r\n\r\nThis is the 0.9.2 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not completely backwards compatible with earlier versions. However, models exported for 0.7.X and 0.8.X should work with this release. **This is a bugfix release and retains compatibility with the 0.9.0 and 0.9.1 models. All model files included here are identical to the ones in the 0.9.0 release.** As with previous releases, this release includes the source code:\r\n\r\n[v0.9.2.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.9.2.tar.gz)\r\n\r\nUnder the [MPL-2.0 license](https:\/\/www.mozilla.org\/en-US\/MPL\/2.0\/). And the acoustic models:\r\n\r\n[deepspeech-0.9.2-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-models.pbmm)\r\n[deepspeech-0.9.2-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-models.tflite)\r\n\r\nIn addition we're releasing experimental Mandarin Chinese acoustic models trained on an internal corpus composed of 2000h of read speech:\r\n\r\n[deepspeech-0.9.2-models-zh-CN.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-models-zh-CN.pbmm)\r\n[deepspeech-0.9.2-models-zh-CN.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-models-zh-CN.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model files with the \".pbmm\" extension are memory mapped and thus memory efficient and fast to load. The model files with the \".tflite\" extension are converted to use TensorFlow Lite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and are more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English with synthetic noise augmentation and the .pbmm model achieves an 7.06% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.9.2-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nThere is also a corresponding scorer for the Mandarin Chinese model:\r\n\r\n[deepspeech-0.9.2-models-zh-CN.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-models-zh-CN.scorer)\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.9.2.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/audio-0.9.2.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files for both the English and Mandarin models:\r\n\r\n[deepspeech-0.9.2-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-checkpoint.tar.gz)\r\n[deepspeech-0.9.2-checkpoint-zh-CN.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.2\/deepspeech-0.9.2-checkpoint-zh-CN.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Add support for Python 3.9 for native client packages ([#3409](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3409))\r\n- Add CI testing for hot word boosting on Java package ([#3410](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3410))\r\n- Add importer for French dataset from Centre de Conf\u00e9rences Pierre Mend\u00e8s-France ([#3438](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3438))\r\n- Add support for ElectronJS v11.0 ([#3441](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3441))\r\n- Correct documentation for needed versions of CUDA for training DeepSpeech ([#3443](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3443))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred as a fine tuning of the previous 0.8.2 checkpoint, with data augmentation options enabled. The following hyperparameters were used for the fine tuning. See the 0.8.2 release notes for the hyperparameters used for the base model.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 200\r\n  * `augment` `pitch[pitch=1~0.1]`\r\n  * `augment` `tempo[factor=1~0.1]`\r\n  * `augment` `overlay[p=0.9,source=${noise},layers=1,snr=12~4]` (where ${noise} is a dataset of Freesound.org background noise recordings)\r\n  * `augment` `overlay[p=0.1,source=${voices},layers=10~2,snr=12~4]` (where ${voices} is a dataset of audiobook snippets extracted from Librivox)\r\n  * `augment` `resample[p=0.2,rate=12000~4000]`\r\n  * `augment` `codec[p=0.2,bitrate=32000~16000]`\r\n  * `augment` `reverb[p=0.2,decay=0.7~0.15,delay=10~8]`\r\n  * `augment` `volume[p=0.2,dbfs=-10~10]`\r\n  * `cache_for_epochs` 10\r\n\r\nThe weights with the best validation loss were selected at the end of 200 epochs using `--noearly_stop`.\r\n\r\nThe optimal `lm_alpha` and `lm_beta` values with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus remain unchanged from the previous release:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\nFor the Mandarin Chinese model, the following values are recommended:\r\n\r\n  * `lm_alpha` 0.6940122363709647\r\n  * `lm_beta` 4.777924224113021\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.2\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7, 3.8 and 3.9) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.2\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x, 14.x and 15.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, 9.1, 9.2, 10.0, 10.1, and 11.0 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.2\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.2\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.2) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.2).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n* TFLite Delegation API is here as a preview: do not expect released models to work out-of-the box, but feedback \/ PRs is welcome.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.2\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.9.2 release\r\n\r\n- Alexandre Lissy\r\n- Catalin Voss\r\n- Dag7\r\n- Rahul Karmakar",
        "3": "# General\r\n\r\nThis is the 0.9.1 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not completely backwards compatible with earlier versions. However, models exported for 0.7.X and 0.8.X should work with this release. **This is a bugfix release and retains compatibility with the 0.9.0 models. All model files included here are identical to the ones in the 0.9.0 release.** As with previous releases, this release includes the source code:\r\n\r\n[v0.9.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.9.1.tar.gz)\r\n\r\nUnder the [MPL-2.0 license](https:\/\/www.mozilla.org\/en-US\/MPL\/2.0\/). And the acoustic models:\r\n\r\n[deepspeech-0.9.1-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-models.pbmm)\r\n[deepspeech-0.9.1-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-models.tflite)\r\n\r\nIn addition we're releasing experimental Mandarin Chinese acoustic models trained on an internal corpus composed of 2000h of read speech:\r\n\r\n[deepspeech-0.9.1-models-zh-CN.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-models-zh-CN.pbmm)\r\n[deepspeech-0.9.1-models-zh-CN.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-models-zh-CN.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model files with the \".pbmm\" extension are memory mapped and thus memory efficient and fast to load. The model files with the \".tflite\" extension are converted to use TensorFlow Lite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and are more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English with synthetic noise augmentation and the .pbmm model achieves an 7.06% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.9.1-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nThere is also a corresponding scorer for the Mandarin Chinese model:\r\n\r\n[deepspeech-0.9.1-models-zh-CN.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-models-zh-CN.scorer)\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.9.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/audio-0.9.1.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files for both the English and Mandarin models:\r\n\r\n[deepspeech-0.9.1-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-checkpoint.tar.gz)\r\n[deepspeech-0.9.1-checkpoint-zh-CN.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.1\/deepspeech-0.9.1-checkpoint.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Fixed problem with documentation build on ReadTheDocs.org ([#3399](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3399))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred as a fine tuning of the previous 0.8.2 checkpoint, with data augmentation options enabled. The following hyperparameters were used for the fine tuning. See the 0.8.2 release notes for the hyperparameters used for the base model.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 200\r\n  * `augment` `pitch[pitch=1~0.1]`\r\n  * `augment` `tempo[factor=1~0.1]`\r\n  * `augment` `overlay[p=0.9,source=${noise},layers=1,snr=12~4]` (where ${noise} is a dataset of Freesound.org background noise recordings)\r\n  * `augment` `overlay[p=0.1,source=${voices},layers=10~2,snr=12~4]` (where ${voices} is a dataset of audiobook snippets extracted from Librivox)\r\n  * `augment` `resample[p=0.2,rate=12000~4000]`\r\n  * `augment` `codec[p=0.2,bitrate=32000~16000]`\r\n  * `augment` `reverb[p=0.2,decay=0.7~0.15,delay=10~8]`\r\n  * `augment` `volume[p=0.2,dbfs=-10~10]`\r\n  * `cache_for_epochs` 10\r\n\r\nThe weights with the best validation loss were selected at the end of 200 epochs using `--noearly_stop`.\r\n\r\nThe optimal `lm_alpha` and `lm_beta` values with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus remain unchanged from the previous release:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\nFor the Mandarin Chinese model, the following values are recommended:\r\n\r\n  * `lm_alpha` 0.6940122363709647\r\n  * `lm_beta` 4.777924224113021\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.1\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.1\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x, 14.x and 15.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, 9.1, 9.2, 10.0 and 10.1 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.1\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.1\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.1) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.1).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n* TFLite Delegation API is here as a preview: do not expect released models to work out-of-the box, but feedback \/ PRs is welcome.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.1\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.9.1 release\r\n\r\n- Alexandre Lissy\r\n",
        "4": "",
        "5": "",
        "6": "# General\r\n\r\nThis is the 0.9.0 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not completely backwards compatible with earlier versions. However, models exported for 0.7.X and 0.8.X should work with this release. As with previous releases, this release includes the source code:\r\n\r\n[v0.9.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.9.0.tar.gz)\r\n\r\nUnder the [MPL-2.0 license](https:\/\/www.mozilla.org\/en-US\/MPL\/2.0\/). And the acoustic models:\r\n\r\n[deepspeech-0.9.0-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-models.pbmm)\r\n[deepspeech-0.9.0-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-models.tflite)\r\n\r\nIn addition we're releasing experimental Mandarin Chinese acoustic models trained on an internal corpus composed of 2000h of read speech:\r\n\r\n[deepspeech-0.9.0-models-zh-CN.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-models-zh-CN.pbmm)\r\n[deepspeech-0.9.0-models-zh-CN.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-models-zh-CN.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model files with the \".pbmm\" extension are memory mapped and thus memory efficient and fast to load. The model files with the \".tflite\" extension are converted to use TensorFlow Lite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and are more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English with synthetic noise augmentation and the .pbmm model achieves an 7.06% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.9.0-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nThere is also a corresponding scorer for the Mandarin Chinese model:\r\n\r\n[deepspeech-0.9.0-models-zh-CN.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-models-zh-CN.scorer)\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.9.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/audio-0.9.0.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files for both the English and Mandarin models:\r\n\r\n[deepspeech-0.9.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-checkpoint.tar.gz)\r\n[deepspeech-0.9.0-checkpoint-zh-CN.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.9.0\/deepspeech-0.9.0-checkpoint.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Fixed incorrect minimum OS version in macOS binaries ([#3259](https:\/\/github.com\/mozilla\/STT\/pull\/3260))\r\n- Fixed bug in metadata output for Python package client ([#3264](https:\/\/github.com\/mozilla\/STT\/pull\/3264))\r\n- Added ElectronJS v9.2 support ([#3266](https:\/\/github.com\/mozilla\/STT\/pull\/3266))\r\n- Improved Bytes output mode documentation\r\n- (Optional) Layer Norm support in training\r\n- Add support for boosting scores for hot words during decoding ([#3297](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3297))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred as a fine tuning of the previous 0.8.2 checkpoint, with data augmentation options enabled. The following hyperparameters were used for the fine tuning. See the 0.8.2 release notes for the hyperparameters used for the base model.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 200\r\n  * `augment` pitch[pitch=1~0.1]\r\n  * `augment` tempo[factor=1~0.1]\r\n  * `augment` overlay[p=0.9,source=${noise},layers=1,snr=12~4] (where ${noise} is a dataset of Freesound.org background noise recordings)\r\n  * `augment` overlay[p=0.1,source=${voices},layers=10~2,snr=12~4] (where ${voices} is a dataset of audiobook snippets extracted from Librivox)\r\n  * `augment` resample[p=0.2,rate=12000~4000]\r\n  * `augment` codec[p=0.2,bitrate=32000~16000]\r\n  * `augment` reverb[p=0.2,decay=0.7~0.15,delay=10~8]\r\n  * `augment` volume[p=0.2,dbfs=-10~10]\r\n  * `cache_for_epochs` 10\r\n\r\nThe weights with the best validation loss were selected at the end of 200 epochs using `--noearly_stop`.\r\n\r\nThe optimal `lm_alpha` and `lm_beta` values with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus remain unchanged from the previous release:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\nFor the Mandarin Chinese model, the following values are recommended:\r\n\r\n  * `lm_alpha` 0.6940122363709647\r\n  * `lm_beta` 4.777924224113021\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.0\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.0\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x, 14.x and 15.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, 9.1 and 9.2 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.0\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.0\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.0) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.9.0).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n* TFLite Delegation API is here as a preview: do not expect released models to work out-of-the box, but feedback \/ PRs is welcome.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.9.0\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.9.0 release\r\n\r\n- Alexandre Lissy\r\n- Anas Abou Allaban\r\n- Bernardo Henz\r\n- Daniel\r\n- Dewi Bryn Jones\r\n- Eren G\u00f6lge\r\n- Francis Tyers\r\n- godeffroy\r\n- Greg Cooke\r\n- imrahul3610\r\n- Jedrzej Beniamin Orbik\r\n- Josh Meyer\r\n- Kelly Davis\r\n- Liezl P\r\n- Michael Stegeman\r\n- Neil Stoker\r\n- Olaf Thiele\r\n- Ptitloup\r\n- Reuben Morais\r\n- Suriyaa Sundararuban\r\n- THCKwarter\r\n- tiagomoraismorgado\r\n- Tilman Kamp",
        "7": "",
        "8": "",
        "9": "",
        "10": "# General\r\n\r\nThis is the 0.8.2 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not completely backwards compatible with earlier versions. However, models exported for 0.7.X should work with this release. As with previous releases, this release includes the source code:\r\n\r\n[v0.8.2.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.8.2.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.8.2-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.2\/deepspeech-0.8.2-models.pbmm)\r\n[deepspeech-0.8.2-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.2\/deepspeech-0.8.2-models.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.8.2-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.2\/deepspeech-0.8.2-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.8.2.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.2\/audio-0.8.2.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.8.2-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.2\/deepspeech-0.8.2-checkpoint.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Fixed incorrect minimum OS version in macOS binaries ([#3259](https:\/\/github.com\/mozilla\/STT\/pull\/3260))\r\n- Fixed bug in metadata output for Python package client ([#3264](https:\/\/github.com\/mozilla\/STT\/pull\/3264))\r\n- Added ElectronJS v9.2 support ([#3266](https:\/\/github.com\/mozilla\/STT\/pull\/3266))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.2\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.2\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x and 14.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, 9.1 and 9.2 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.2\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.2\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.8.2) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.8.2).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n* TFLite Delegation API is here as a preview: do not expect released models to work out-of-the box, but feedback \/ PRs is welcome.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.2\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.8.2 release\r\n\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Ptitloup](https:\/\/github.com\/tilmankamp)\r\n",
        "11": "",
        "12": "",
        "13": "# General\r\n\r\nThis is the 0.8.1 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not completely backwards compatible with earlier versions. However, models exported for 0.7.X should work with this release. As with previous releases, this release includes the source code:\r\n\r\n[v0.8.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.8.1.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.8.1-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.1\/deepspeech-0.8.1-models.pbmm)\r\n[deepspeech-0.8.1-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.1\/deepspeech-0.8.1-models.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.8.1-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.1\/deepspeech-0.8.1-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.8.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.1\/audio-0.8.1.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.8.1-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.1\/deepspeech-0.8.1-checkpoint.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Fixed references to older models in the docs  and swift code ([#3216](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3216))\r\n- Fixed incorrect linkage, -shared was forced ([#3207](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3207))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.1\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.1\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x and 14.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, and 9.1 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.1\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.1\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.8.1) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.8.1).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.1\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.8.1 release\r\n\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [Daniel](https:\/\/github.com\/DanBmh)\r\n* [Karan Sagar](https:\/\/github.com\/karansag)\r\n* [Qian Xiao](https:\/\/github.com\/pbxqdown)\r\n* [Carlos Fonseca](https:\/\/github.com\/carlfm01)\r\n* [Erik Ziegler](https:\/\/github.com\/erksch)\r\n* [Karthikeyan Singaravelan](https:\/\/github.com\/tirkarthi)\r\n",
        "14": "",
        "15": "",
        "16": "# General\r\n\r\nThis is the 0.8.0 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not completely backwards compatible with earlier versions. However, models exported for 0.7.X should work with this release. As with previous releases, this release includes the source code:\r\n\r\n[v0.8.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.8.0.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.8.0-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.0\/deepspeech-0.8.0-models.pbmm)\r\n[deepspeech-0.8.0-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.0\/deepspeech-0.8.0-models.tflite)\r\n\r\nall under the MPL-2.0 license.\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nNote that the model currently performs best in low-noise environments with clear recordings and has a bias towards US male accents. This does not mean the model cannot be used outside of these conditions, but that accuracy may be lower. Some users may need to train the model further to meet their intended use-case.\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.8.0-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.0\/deepspeech-0.8.0-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases and which is also under the MPL-2.0 license.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.8.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.0\/audio-0.8.0.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.8.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.8.0\/deepspeech-0.8.0-checkpoint.tar.gz)\r\n\r\nwhich are under the MPL-2.0 license and can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Removed scorer file from Git LFS ([#3192](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3192))\r\n- Added iOS microphone streaming ([#3191](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3191))\r\n- Added ability to reverse data set order to quickly probe OOM conditions ([#3177](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3177))\r\n- Build and publish iOS framework in GitHub release files ([#3173](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3173))\r\n- Added iOS support ([#3150](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3150))\r\n- Add csv output to SDB building ([#3147](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3147))\r\n- Add augmentation support to SDB building ([#3145](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3145))\r\n- Fixed some style inconsistencies in Java bindings ([#3135](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3135))\r\n- Added methods to check for label presence in the Alphabet ([#3131](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3131))\r\n- Fixed some regressions from Alphabet refactoring ([#3125](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3125))\r\n- Re-wrote generate_package.py in C++ to avoid training dependencies ([#3113](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3113))\r\n- Added building of kenlm in training container image ([#3108](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3108))\r\n- Added TensorFlow as a submodule ([#3107](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3107))\r\n- Use TensorFlow r2.2 and build TFLite with Ruy (enables threaded computations on TFLite models) ([#2952](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2952))\r\n- Enable TFLite [delegate](https:\/\/www.tensorflow.org\/lite\/performance\/delegates) support ([#3100](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3100))\r\n- Add UWP Nuget packing support ([#3100](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3100))\r\n- Added warp augmentation ([#3091](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3091))\r\n- Fix of overlay augmentation hang after first epoch ([#3090](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3090))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to some previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.0\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.0\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x and 14.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0, 9.0, and 9.1 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.0\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.0\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.8.0) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.8.0).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14, and 10.15\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU (at least AVX\/FMA) + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3, Pi 4\r\n* Linux\/ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android (7.0-11.0) bindings (+ demo app). Tested on Google Pixel 2 ; Sony Xperia Z Premium ; Nokia 1.3, TF Lite model only.\r\n* iOS with Swift bindings (experimental). Tested on iPhone Xs.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.8.0\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.8.0 release\r\n\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [Daniel](https:\/\/github.com\/DanBmh)\r\n* [Karan Sagar](https:\/\/github.com\/karansag)\r\n* [Qian Xiao](https:\/\/github.com\/pbxqdown)\r\n* [Carlos Fonseca](https:\/\/github.com\/carlfm01)\r\n* [Erik Ziegler](https:\/\/github.com\/erksch)\r\n* [Karthikeyan Singaravelan](https:\/\/github.com\/tirkarthi)\r\n",
        "17": "",
        "18": "",
        "19": "",
        "20": "",
        "21": "",
        "22": "",
        "23": "",
        "24": "",
        "25": "",
        "26": "",
        "27": "",
        "28": "",
        "29": "",
        "30": "",
        "31": "",
        "32": "",
        "33": "",
        "34": "",
        "35": "",
        "36": "",
        "37": "",
        "38": "",
        "39": "",
        "40": "",
        "41": "# General\r\n\r\nThis is the 0.7.4 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with version 0.6.1 or earlier versions. **This is a bugfix release and retains compatibility with the 0.7.0 models. All model files included here are identical to the ones in the 0.7.0 release.** As with previous releases, this release includes the source code:\r\n\r\n[v0.7.4.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.7.4.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.7.4-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.4\/deepspeech-0.7.4-models.pbmm)\r\n[deepspeech-0.7.4-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.4\/deepspeech-0.7.4-models.tflite).\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.7.4-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.4\/deepspeech-0.7.4-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.7.4.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.4\/audio-0.7.4.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.7.4-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.4\/deepspeech-0.7.4-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Fix csv.DictWriter configuration on Windows in some importers ([#3045](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3045))\r\n- Reduce number of users of VERSION and GRAPH_VERSION symlinks to fix issues on Windows ([#3043](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3043))\r\n- Fix bug in ds_ctcdecoder SWIG definition which was causing wrapper objects to be leaked ([#3049](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3049))\r\n- Add support for read-only validation metrics (not affecting best validation checkpoint logic) ([#3051](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3051))\r\n- Fix some importers to report total **imported** audio duration alongside total input audio duration ([#3054](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3054))\r\n- Separate Dockerfile into one for training and one for building native client related tools ([#3060](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3060))\r\n- Add list of supported platforms to ReadTheDocs ([#3065](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3065))\r\n- Added third-party bindings for the Nim language ([#3076](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3076))\r\n- Avoid reinstalling TensorFlow package from PyPI when using Docker bases that already come with it ([#3072](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3072))\r\n- Refactor artifact caching mechanism in CI ([#3069](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3069))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.4\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.4\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x and 14.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0 and 9.0 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.4\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.4\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.4) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.4).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14 and 10.15\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3 + Raspberry Pi 4\r\n* ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.4\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.7.4 release\r\n\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Anubhav](https:\/\/github.com\/eagledot)\r\n* [Daniel](https:\/\/github.com\/DanBmh)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Marek Grzegorek](https:\/\/github.com\/marekjg)\r\n* [Adarsh Shetty](https:\/\/github.com\/ObliviousParadigm)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [RickyChan](https:\/\/github.com\/ricky-ck-chan)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n",
        "42": "",
        "43": "# General\r\n\r\nThis is the 0.7.3 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with version 0.6.1 or earlier versions. **This is a bugfix release and retains compatibility with the 0.7.0 models. All model files included here are identical to the ones in the 0.7.0 release.** As with previous releases, this release includes the source code:\r\n\r\n[v0.7.3.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.7.3.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.7.3-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.3\/deepspeech-0.7.3-models.pbmm)\r\n[deepspeech-0.7.3-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.3\/deepspeech-0.7.3-models.tflite).\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.7.3-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.3\/deepspeech-0.7.3-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.7.3.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.3\/audio-0.7.3.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.7.3-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.3\/deepspeech-0.7.3-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n- Bug fix - test_csvs argument was ignored ([#2994](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2994))\r\n- Convert path to str to fix Python 3.5 compat ([#3025](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3025))\r\n- Added support for NodeJS v14 and ElectronJS v9.0 ([#3027](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3027))\r\n- Improve error handling around Scorer ([#2998](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2998))\r\n- Windows support in setup.py decoder wheel installation ([#3001](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3001))\r\n- Fix JS IntermediateDecodeWithMetadata binding ([#3011](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3011))\r\n- Switch index.js to TypeScript ([#3012](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3012))\r\n- Return raw scores in confidence value ([#3021](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/3021))\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.3\/USING.html#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.3\/USING.html#using-the-node-js-electron-js-package) (Versions 10.x, 11.x, 12.x, 13.x and 14.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, 8.0 and 9.0 are also supported\r\n\r\n* [C](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.3\/C-Examples.html) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.3\/USING.html#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.3) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.3).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14 and 10.15\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3 + Raspberry Pi 4\r\n* ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.3\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.7.3 release\r\n* Alexandre Lissy\r\n* Greg Richardson\r\n* J\u0119drzej Beniamin Orbik\r\n* Kelly Davis\r\n* Reuben Morais\r\n* Shubham Kumar\r\n* Tilman Kamp",
        "44": "",
        "45": "",
        "46": "",
        "47": "# General\r\n\r\nThis is the 0.7.1 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with version 0.6.1 or earlier versions. **This is a bugfix release and retains compatibility with the 0.7.0 models. All model files included here are identical to the ones in the 0.7.0 release.** As with previous releases, this release includes the source code:\r\n\r\n[v0.7.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.7.1.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.7.1-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.1\/deepspeech-0.7.1-models.pbmm)\r\n[deepspeech-0.7.1-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.1\/deepspeech-0.7.1-models.tflite).\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.7.1-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.1\/deepspeech-0.7.1-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.7.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.1\/audio-0.7.1.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.7.1-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.1\/deepspeech-0.7.1-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Moved all usage documentation to [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io), where they're properly versioned and by default redirected to the latest stable release. ([#2949](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2949))\r\n- Statically link libsox on macOS native client so users don't have to install sox. ([#2951](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2951))\r\n- Fix a bug where JavaScript binding was not returning the Stream wrapper in `Model.createStream`. ([#2957](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2957))\r\n- Fix a bug where `DS_EnableExternalScorer` (and equivalent bindings) did not properly handle errors and left a partially initializer Scorer in place. ([#2970](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2970)).\r\n- Fix a bug in the Python client when overriding the default beam width with the `--beam_width` flag. ([#2976](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2976))\r\n- Fix a bug in the JavaScript binding where strict mode was violated in `Stream.finishStreamWithMetadata`. ([#2980](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2980) \/ [#2981](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2981)).\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN RNN.\r\n\r\nIn contrast to previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.1\/USING.rst#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.1\/USING.rst#using-the-nodejs-package) (Versions 10.x, 11.x, 12.x, and 13.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, and 8.0 are also supported\r\n\r\n* [C](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.1\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.1\/USING.rst#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.1) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.1).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14 and 10.15\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3 + Raspberry Pi 4\r\n* ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.1\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.7.1 release\r\n* Alexandre Lissy\r\n* David Gauchard\r\n* Josh Meyer\r\n* Julien Schueller\r\n* Matt McCartney\r\n* Reuben Morais\r\n* Tilman Kamp\r\n* \u0141ukasz Wa\u0142ejko",
        "48": "",
        "49": "",
        "50": "",
        "51": "",
        "52": "# General\r\n\r\nThis is the 0.7.0 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with version 0.6.1 or earlier versions. So when updating one will have to update code and models. As with previous releases, this release includes the source code:\r\n\r\n[v0.7.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.7.0.tar.gz)\r\n\r\nand the acoustic models:\r\n\r\n[deepspeech-0.7.0-models.pbmm](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.0\/deepspeech-0.7.0-models.pbmm)\r\n[deepspeech-0.7.0-models.tflite](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.0\/deepspeech-0.7.0-models.tflite).\r\n\r\nThe model with the \".pbmm\" extension is memory mapped and thus memory efficient and fast to load. The model with the \".tflite\" extension is converted to use TFLite, has [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and is more suitable for resource constrained environments.\r\n\r\nThe acoustic models were trained on American English and the pbmm model achieves an 5.97% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12).\r\n\r\nIn addition we release the scorer:\r\n\r\n[deepspeech-0.7.0-models.scorer](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.0\/deepspeech-0.7.0-models.scorer)\r\n\r\nwhich takes the place of the language model and trie in older releases.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.7.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.0\/audio-0.7.0.tar.gz)\r\n\r\nwhich can be used to test the engine, and checkpoint files:\r\n\r\n[deepspeech-0.7.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.7.0\/deepspeech-0.7.0-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n- Added Multi-stream .NET support[[1](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2679)].\r\n- **Fixed upper frequency limit when computing MFCC's**[[2](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2688)].\r\n- Remove `benchmark_nc` as it was not used[[3](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2704)].\r\n- Added TFLite-specific NPM package[[4](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2705)].\r\n- Added TFLite NuGet package[[5](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2706)].\r\n- **Added Sample DBs, a new format for training data that allows for much improved training speeds**[[6](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2723)].\r\n- Re-worked the reporting of WER during model evaluation[[7](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2724)].\r\n- Fixed incorrect decoding format in .NET[[8](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2726)].\r\n- Embedded beam width in model and made the parameter optional in API[[9](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2744)].\r\n- **Added support for transfer learning as described in Chapter 8 of Josh Meyer's PhD thesis**[[10](http:\/\/jrmeyer.github.io\/misc\/MEYER_dissertation_2019.pdf)][[11](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2763)][[12](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2912)].\r\n- Added support for ElectronJS v8.0[[13](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2764\/files)].\r\n- Added optimizer to select the optimal lm_alpha + lm_beta[[14](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2783)][[19](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2826)].\r\n- **Exposed multiple transcriptions in \"WithMetadata\" API**[[16](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2792)].\r\n- **New packaging format for external scorer (previously lm.binary and trie files)**[[26](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2681)].\r\n- Exposed error codes in a human readable form[[17](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2794)][[18](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2806)].\r\n- Bumped dependency to TensorFlow 1.15.2[[20](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2835)].\r\n- **Re-packaged training code to be installable simplifying training setup**[[21](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2856)][[22](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2871)].\r\n- Added recursive transcription of directories to transcribe.py[[23](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2879)].\r\n- Added support for TypeScript[[24](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2882\/files)].\r\n- Fixed bug in computation of initial timestamp[[25](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2915\/files)].\r\n- **Moved Stream-relative functions to be methods in the Stream object in Python and JavaScript bindings**[[27](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2792)].\r\n\r\n# Training Regimen + Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the training regimen, hardware used (a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM), and our use of cuDNN.\r\n\r\nIn contrast to previous releases, training for this release occurred in several phases each phase with a lower learning rate than the phase before it.\r\n\r\nThe initial phase used the hyperparameters:\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.40\r\n  * `epochs` 125\r\n\r\nThe weights with the best validation loss were selected at the end of 125 epochs using `--noearly_stop`.\r\n\r\nThe second phase was started using the weights with the best validation loss from the previous phase. This second phase used the same hyperparameters as the first but with the following changes:\r\n\r\n  * `learning_rate` 0.00001\r\n  * `epochs` 100\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`.\r\n\r\nLike the second, the third phase was started using the weights with the best validation loss from the previous phase. This third phase used the same hyperparameters as the second but with the following changes:\r\n\r\n  * `learning_rate` 0.000005\r\n\r\nThe weights with the best validation loss were selected at the end of 100 epochs using `--noearly_stop`. The model selected under this process was trained for a sum total of 732522 steps over all phases.\r\n\r\nSubsequent to this the `lm_optimizer.py` was used with the following parameters:\r\n\r\n  * `lm_alpha_max` 5\r\n  * `lm_beta_max` 5\r\n  * `n_trials` 2400\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n\r\nto determine the optimal `lm_alpha` and `lm_beta` with respect to the [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus. This resulted in:\r\n\r\n  * `lm_alpha` 0.931289039105002\r\n  * `lm_beta` 1.1834137581510284\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.0\/USING.rst#using-the-python-package) (Versions 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  pip install deepspeech-tflite\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.0\/USING.rst#using-the-nodejs-package) (Versions 10.x, 11.x, 12.x, and 13.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n  On Linux (AMD64), macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n  ```bash\r\n  npm install deepspeech-tflite\r\n  ```\r\n\r\n* ElectronJS versions 5.0, 6.0, 6.1, 7.0, 7.1, and 8.0 are also supported\r\n\r\n* [C](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.0\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.7.0\/USING.rst#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.0) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.7.0).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14 and 10.15\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3 + Raspberry Pi 4\r\n* ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only.\r\n\r\n# Documentation\r\n\r\nDocumentation is available on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/v0.7.0\/).\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [Matrix](https:\/\/chat.mozilla.org\/#\/room\/#machinelearning:mozilla.org) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning:mozilla.org` channel on Mozilla Matrix; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.7.0 release\r\n* [Alex Cannan](https:\/\/github.com\/alexcannan)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Anas Abou Allaban](https:\/\/github.com\/piraka9011)\r\n* [Caleb Moses](https:\/\/github.com\/mathematiguy)\r\n* [Carlos Fonseca M](https:\/\/github.com\/carlfm01)\r\n* [Christian Eberhardt](https:\/\/github.com\/chrillemanden)\r\n* [dabinat](https:\/\/github.com\/dabinat)\r\n* [DanBmh](https:\/\/github.com\/DanBmh)\r\n* [Daniel](mailto:daniel@mail.de)\r\n* [Francis Tyers](https:\/\/github.com\/ftyers)\r\n* [Jedrzej Beniamin Orbik](https:\/\/github.com\/Jendker)\r\n* [Jim Regan](https:\/\/github.com\/jimregan)\r\n* [Josh Meyer](https:\/\/github.com\/JRMeyer)\r\n* [juandspy](https:\/\/github.com\/juandspy)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [madprogramer](https:\/\/github.com\/madprogramer)\r\n* [Norman Koch](https:\/\/github.com\/NormanTUD)\r\n* [PedroDKE](https:\/\/github.com\/PedroDKE)\r\n* [Pratik Raj](https:\/\/github.com\/Rajpratik71)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Richard Hamnett](https:\/\/github.com\/rhamnett)\r\n* [Ryoji Yoshida](https:\/\/github.com\/ryojiysd)\r\n* [Shubham Kumar](https:\/\/github.com\/imskr)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n\r\n",
        "53": "",
        "54": "",
        "55": "",
        "56": "",
        "57": "",
        "58": "# General\r\n\r\nThis is the 0.6.1 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with version 0.5.1 or earlier versions. So when updating one will have to update code and models. As with previous releases, this release source code:\r\n\r\n[v0.6.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.6.1.tar.gz)\r\n\r\nand a model\r\n\r\n[deepspeech-0.6.1-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.6.1\/deepspeech-0.6.1-models.tar.gz) (This is identical to the 0.6.0 model).\r\n\r\ntrained on American English which achieves an 7.5% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12). Models with a \"*.pbmm\" extension are memory mapped and much more memory efficient, as well as faster to load. Models with the \".tflite\" extension are converted to use with TFLite and have [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and are more suitable for resource constrained environments.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.6.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.6.1\/audio-0.6.1.tar.gz)\r\n\r\nwhich can be used to test the engine; and checkpoint files\r\n\r\n[deepspeech-0.6.1-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.6.1\/deepspeech-0.6.1-checkpoint.tar.gz) (This is identical to the 0.6.0 checkpoint, except the missing alphabet.txt file is now included.)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\nDeepSpeech 0.6.1 is a patch release that addresses some minor points surfaced after the 0.6.0 release:\r\n\r\n- Fixed a bug where silence was incorrectly transcribed as \"i\", \"a\" or (rarely) other one letter transcriptions.\r\n- Fixed a bug where the TFLite version of the model was exported with a mismatched `forget_bias` setting.\r\n- Fixed some broken links in the documentation and the PyPI package listing.\r\n- Build and package TFLite version of the Python package for desktop platforms as `deepspeech-tflite`.\r\n- Move examples to a separate repository for easier maintenance: https:\/\/github.com\/mozilla\/DeepSpeech-examples.\r\n- Remove outdated remark about the performance of `DS_IntermediateDecode` from the docs.\r\n- Added third party bindings for the [V programming language](https:\/\/vlang.io\/)\r\n- Fixed incorrect shape handling in online augmentation code.\r\n- Minor fixes to documentation text and CLI flag help texts.\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM. These are identical to the 0.6.0 release.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.20\r\n  * `epoch` 75\r\n  * `lm_alpha` 0.75\r\n  * `lm_beta` 1.85\r\n\r\nThe weights with the best validation loss were selected at the end of 75 epochs using `--noearly_stop`, and the selected model was trained for 233784 steps. In addition the training used the `--use_cudnn_rnn` flag.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.1\/USING.rst#using-the-python-package) (Versions 2.7, 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\nOn Linux, macOS and Windows, the DeepSpeech package does not use TFLite by default. A TFLite version of the package on those platforms is available as:\r\n```bash\r\npip install deepspeech-tflite\r\n```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.1\/USING.rst#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x, 10.x, 11.x, 12.x and 13.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* ElectronJS versions 3.1, 4.0, 4.1, 5.0, 6.0, 7.0 and 7.1 are also supported\r\n\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.1\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.1\/USING.rst#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.6.1) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.6.1).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n* [V](https:\/\/github.com\/thecodrr\/vspeech) which is installed by following the instructions on the external Vlang repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14 and 10.15\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3 + Raspberry Pi 4\r\n* ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only.\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.6.1 release\r\n\r\n* [Abdullah Atta](https:\/\/github.com\/thecodrr)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Carlos Fonseca](https:\/\/github.com\/carlfm01)\r\n* [J\u0119drzej Beniamin Orbik](https:\/\/github.com\/Jendker)\r\n* [Josh Meyer](https:\/\/github.com\/JRMeyer)\r\n* [Kathy Reid](https:\/\/github.com\/KathyReid)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Pietro](https:\/\/github.com\/pietrop)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [Yi-Hua Chiu](https:\/\/github.com\/mychiux413)\r\n",
        "59": "",
        "60": "",
        "61": "",
        "62": "",
        "63": "",
        "64": "",
        "65": "",
        "66": "# General\r\n\r\nThis is the 0.6.0 release of Deep Speech, an open speech-to-text engine. In accord with [semantic versioning](https:\/\/semver.org\/), this version is not backwards compatible with version 0.5.1 or earlier versions. So when updating one will have to update code and models. As with previous releases, this release includes trained models and source code.\r\n\r\n[v0.6.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.6.0.tar.gz)\r\n\r\nand a model\r\n\r\n[deepspeech-0.6.0-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.6.0\/deepspeech-0.6.0-models.tar.gz)\r\n\r\ntrained on American English which achieves an 7.5% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12). Models with a \"*.pbmm\" extension are memory mapped and much more memory efficient, as well as faster to load. Models with the \".tflite\" extension are converted to use with TFLite and have [post-training quantization](https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization) enabled, and are more suitable for resource constrained environments.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.6.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.6.0\/audio-0.6.0.tar.gz)\r\n\r\nwhich can be used to test the engine; and checkpoint files\r\n\r\n[deepspeech-0.6.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.6.0\/deepspeech-0.6.0-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\nDeepSpeech 0.6.0 includes a number of significant changes. These changes break backwards compatibility with code targeting older releases as well as training or exporting older checkpoints. For details on the changes, see below:\r\n\r\n- **API** - We have cleaned up several inconsistencies in our API, making function names more uniform and removing unused parameters. We have included a simple [wrapper header](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/v0.6.0\/native_client\/deepspeech_compat.h) that can be used by users of the C API if they absolutely cannot change their code. It's not a complicated upgrade, here's a summary:\r\n    - `DS_CreateModel` arguments `alphabet`, `n_cep` and `n_context` removed. (Now retrieved from the model file).\r\n    - `DS_EnableDecoderWithLM` argument `alphabet` removed. (Alphabet retrieved from the model is used).\r\n    - `DS_SetupStream` renamed to `DS_CreateStream` and no longer takes a sample rate parameter.\r\n    - `DS_DestroyModel` renamed to `DS_FreeModel`\r\n    - `DS_DiscardStream` renamed to `DS_FreeStream`\r\n    - `DS_SpeechToText` and `DS_SpeechToTextWithMetadata` no longer take a sample rate parameter.\r\n    - The equivalent methods in the language bindings have also been updated.\r\n- **Checkpoints** - With TF 1.14, we have added CuDNN RNN support to our training graph, which improves training performance significantly. We've seen improvements on the order of 2x faster training time per epoch. The required training graph changes breaks loading older checkpoints, due to differences in the computation performed by CudnnLSTM.\r\n  - Note that mixing CuDNN and non-CuDNN checkpoints requires some care: a CuDNN checkpoint can't be continued normally on a non-CuDNN setup, you'll need to use the `--cudnn_checkpoint` flag to load it and it'll re-initialize the optimizer momentum variables for the RNN weights from scratch.\r\n- **Exported Model** We've fixed a bug where trying to interleave multiple streams with the same Model instance would lead to non-deterministic behavior, as all streams were sharing the same LSTM state between passes through the acoustic model. This was fixed in [#2146](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2146). The fix required changing the inference graph, so trying to load a 0.5.1 model with a newer client would lead to errors. We bumped the graph version accordingly so that clients will fail early.\r\n- **Dependencies** - We have updated our dependencies from TensorFlow 1.13.1 in v0.5.1 to TensorFlow 1.14.0. Make sure you always use the correct TensorFlow version for the version of DeepSpeech you're using.\r\n- **Trie** - We switched to a different data structure for the language model trie file, so that the file could be memory mapped when loading. The older format is no longer supported, so we bumped the version number accordingly. In order to create a trie file that's compatible with the newer version, you have to run an updated `generate_trie` again with your `lm.binary` file. Note that just the trie format changed, the main LM binary file doesn't require changes.\r\n- **Trie Loading** - We have changed the mode of loading the LM trie file to be lazier, which improves memory utilization and latency for the first inference request after creating a model. See discussion and some measurements in [the issue](https:\/\/github.com\/mozilla\/DeepSpeech\/issues\/2384).\r\n- **Language Model** - We have updated the language model by filtering out uncommon words. It now contains only the top 500k words from the text it was trained on. Furthermore, we have pruned it for singletons of order three and higher. (In version 0.5.1 we pruned the language model for singletons of order four and higher.) These together half the size of the language model, taking it from about 1800MB in Deep Speech 0.5.1 to about 900MB in Deep Speech 0.6.0 with little to no impact on word error rate (WER).\r\n- **Data Augmentation** - Several online data augmentation techniques have been contributed. See the [PR here](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2352) and some documentation was [added to the README](https:\/\/github.com\/mozilla\/DeepSpeech\/commit\/58fdd55eea951dabd91a1158207f2228d745e657) as well.\r\n- **Documentation** - We have refactored our documentation to merge docs for the different language bindings under a single resource. The new docs can be seen on [deepspeech.readthedocs.io](https:\/\/deepspeech.readthedocs.io\/en\/latest).\r\n- **Tool for bulk transcription** - We added a [tool](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/transcribe.py) for bulk transcribing large audio files.\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 Quadro RTX 6000 GPUs each with 24GB of VRAM.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/), [Common Voice English](https:\/\/voice.mozilla.org\/datasets), and approximately 1700 hours of transcribed WAMU (NPR) radio shows explicitly licensed to use as training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 128\r\n  * `dev_batch_size` 128\r\n  * `test_batch_size` 128\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.20\r\n  * `epoch` 75\r\n  * `lm_alpha` 0.75\r\n  * `lm_beta` 1.85\r\n\r\nThe weights with the best validation loss were selected at the end of 75 epochs using `--noearly_stop`, and the selected model was trained for 233784 steps. In addition the training used the `--use_cudnn_rnn` flag.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.0\/USING.rst#using-the-python-package) (Versions 2.7, 3.5, 3.6, 3.7 and 3.8) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.0\/USING.rst#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x, 10.x, 11.x, 12.x and 13.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* ElectronJS versions 3.1, 4.0, 4.1, 5.0, 6.0, 7.0 and 7.1 are also supported\r\n\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.0\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/tree\/v0.6.0\/USING.rst#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.6.0) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.6.0).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support, requires `Redistribuable Visual C++ 2015 Update 3 (64-bits)` for runtime).\r\n* OS X 10.10, 10.11, 10.12, 10.13, 10.14 and 10.15\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Buster on Raspberry Pi 3 + Raspberry Pi 4\r\n* ARM64 built against Debian\/ARMbian Buster and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only.\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.6.0 release\r\n\r\n* [aayagar001](https:\/\/github.com\/aayagar001)\r\n* [alchemi5t](https:\/\/github.com\/alchemi5t)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Bernardo Henz](https:\/\/github.com\/bernardohenz)\r\n* [bjorn](https:\/\/github.com\/bjornbytes)\r\n* [bprfh](https:\/\/github.com\/bprfh)\r\n* [Carlos Fonseca](https:\/\/github.com\/carlfm01)\r\n* [Chirag Ahuja](https:\/\/github.com\/cahuja1992)\r\n* [dabinat](https:\/\/github.com\/dabinat)\r\n* [Dustin J. Mitchell](https:\/\/github.com\/djmitche)\r\n* [Francis Tyers](https:\/\/github.com\/ftyers)\r\n* [Josh Meyer](https:\/\/github.com\/JRMeyer)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Li Li](https:\/\/github.com\/eggonlea)\r\n* [Mahmoud Gamal](https:\/\/github.com\/Jemyz)\r\n* [Miles Crabill](https:\/\/github.com\/milescrabill)\r\n* [mone27](https:\/\/github.com\/mone27)\r\n* [Murcurio](https:\/\/github.com\/Murcurio)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Richard Hamnett](https:\/\/github.com\/rhamnett)\r\n* [Robert Gale](https:\/\/github.com\/rcgale)\r\n* [Sam Safaei](https:\/\/github.com\/safa0)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [Vinh Nguyen](https:\/\/github.com\/vinhngx)\r\n",
        "67": "",
        "68": "",
        "69": "",
        "70": "",
        "71": "",
        "72": "",
        "73": "",
        "74": "",
        "75": "# General\r\n\r\nThis is the 0.5.1 release of __Deep Speech__, an open speech-to-text engine. This is a bug-fix release that is backwards compatible with models and checkpoints from 0.5.0. Thanks to [Li Li](https:\/\/github.com\/eggonlea) for identifying and helping fix these bugs. This release includes source code\r\n\r\n[v0.5.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.5.1.tar.gz)\r\n\r\nand a trained model\r\n\r\n[deepspeech-0.5.1-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.5.1\/deepspeech-0.5.1-models.tar.gz) (identical to 0.5.0 models)\r\n\r\ntrained on American English which achieves an 8.22% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12). Models with a \"*.pbmm\" extension are memory mapped and much more memory efficient, as well as faster to load. Models with the \".tflite\" extension are converted to use with TFLite and have post-training quantization enabled, and are more suitable for resource constrained environments.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.5.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.5.1\/audio-0.5.1.tar.gz)\r\n\r\nwhich can be used to test the engine; and checkpoint files\r\n\r\n[deepspeech-0.5.1-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.5.1\/deepspeech-0.5.1-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n* Fixed a bug where evaluate_tflite.py would not correctly take into account all batches when computing the final WER report (#2168)\r\n* Add option to C++ binary to print intermediate transcripts during streaming (#2181)\r\n* Fixed a bug where calling DS_IntermediateDecode during streaming would negatively affect the final transcript for the stream (#2184)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 TitanX Pascal GPUs (12GB of VRAM).\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), and [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpora.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 24\r\n  * `dev_batch_size` 48\r\n  * `test_batch_size` 48\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.15\r\n  * `epoch` 75\r\n  * `lm_alpha` 0.75\r\n  * `lm_beta` 1.85\r\n\r\nThe weights with the best validation loss were selected at the end of the 75 epochs using `--noearly_stop`. The selected model was trained for 467356 steps.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 3.4, 3.5, 3.6 and 3.7) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x, 10.x, 11.x, and 12.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* ElectronJS versions 3.1, 4.0, 4.1, 5.0 are also supported\r\n\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.5.1) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.5.1).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* Windows 8.1, 10, and Server 2012 R2 64-bits (Needs at least AVX support).\r\n* OS X 10.10, 10.11, 10.12, 10.13 and 10.14\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Stretch on Raspberry Pi 3\r\n* ARM64 built against Debian\/ARMbian Stretch and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only\r\n\r\n# Known Issues\r\n\r\n* Feature caching speeds training but increases memory usage\r\n* Current `v2 TRIE` handling still triggers ~600MB memory usage\r\n* Code not yet thread safe, having multiple concurrent streams tied to the same model leads to bad transcriptions.\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.5.1 release\r\n\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Li Li](https:\/\/github.com\/eggonlea)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n",
        "76": "",
        "77": "# General\r\n\r\nThis is the 0.5.0 release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n[v0.5.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.5.0.tar.gz)\r\n\r\nand a trained model\r\n\r\n[deepspeech-0.5.0-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.5.0\/deepspeech-0.5.0-models.tar.gz)\r\n\r\ntrained on American English which achieves an 8.22% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12). Models with a \"*.pbmm\" extension are memory mapped and much more memory efficient, as well as faster to load. Models with the \".tflite\" extension are converted to use with TFLite and have post-training quantization enabled, and are more suitable for resource constrained environments.\r\n\r\nWe also include example audio files:\r\n\r\n[audio-0.5.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.5.0\/audio-0.5.0.tar.gz)\r\n\r\nwhich can be used to test the engine; and checkpoint files\r\n\r\n[deepspeech-0.5.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.5.0\/deepspeech-0.5.0-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n* [Python 2.7 is no longer supported for training]()\r\n* [Update decoder parameter names in native client](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1834)\r\n* [Proper DeepSpeech error codes](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1845)\r\n* [**Produce Maven Bundle and upload to bintray**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1848)\r\n* [Add TFLite accuracy estimation tool](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1854)\r\n* [Fix invalid characters in (Windows) speech to text result](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1857)\r\n* [Added importer for Common Voice v2 corpora](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1860)\r\n* [Added alphabet generation utility](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1874)\r\n* [Enabled TFLite post-training quantization](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1878)\r\n* [**Enabled official Windows support**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1895)\r\n* [Added simple nodejs example](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1900)\r\n* [Improved Nodejs streaming inference with VAD and FFmpeg](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1915)\r\n* [**Implement input pipeline with tf.data API**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1919)\r\n* [Use tf.lite.TFLiteConverter to create tflite model](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1939)\r\n* [**Add Windows NuGet upload of Deep Speech**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1949)\r\n* [**Update to TensorFlow v1.13**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1957)\r\n* [**Add NET Framework targets to NuGet package**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1958)\r\n* [**Added Windows support for npm package**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1967) \r\n* [**Added output of word timings**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1974)\r\n* [**Removed distributed training support**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1988)\r\n* [Add Mozilla Code of Conduct](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1994)\r\n* [**Exposed letter and word timing information**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/1995)\r\n* [**Windows Python bindings**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2003)\r\n* [Embed\/read more metadata in exported model](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2007)\r\n* [**Exposed transcription probability information**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2012)\r\n* [**Add Windows Python packages to PyPI upload tasks**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2018)\r\n* [**Expose extended metadata information to bindings**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2022)\r\n* [Build for ElectronJS](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2035)\r\n* [Perform separate validation and test epochs per dataset when multiple files are specified](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2038)\r\n* [Added LinguaLibre importer](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2068)\r\n* [**Add NodeJS v12 support**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2073)\r\n* [Added AISHELL dataset importer](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2086)\r\n* [Unmangled Exported symbols](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2105)\r\n* [Moved to SWIG 4.0.0](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2107)\r\n* [**Enhanced CTC decoder to stream along with the RNN**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2121)\r\n* [**Use separate execution plans for acoustic model and feature computation**](https:\/\/github.com\/mozilla\/DeepSpeech\/pull\/2141)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 TitanX Pascal GPUs (12GB of VRAM).\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), and [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean dev corpora.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 24\r\n  * `dev_batch_size` 48\r\n  * `test_batch_size` 48\r\n  * `n_hidden` 2048\r\n  * `learning_rate` 0.0001\r\n  * `dropout_rate` 0.15\r\n  * `epoch` 75\r\n  * `lm_alpha` 0.75\r\n  * `lm_beta` 1.85\r\n\r\nThe weights with the best validation loss were selected at the end of the 75 epochs using `--noearly_stop`. The selected model was trained for 467356 steps.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 3.4, 3.5, 3.6 and 3.7) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x, 10.x, 11.x, and 12.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* ElectronJS versions 3.1, 4.0, 4.1, 5.0 are also supported\r\n\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\n* [.NET](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.5.0) which is installed by following the instructions on the [NuGet package page](https:\/\/www.nuget.org\/packages\/DeepSpeech\/0.5.0).\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.10, 10.11, 10.12, 10.13 and 10.14\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Stretch on Raspberry Pi 3\r\n* ARM64 built against Debian\/ARMbian Stretch and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only\r\n\r\n# Known Issues\r\n\r\n* Feature caching speeds training but increases memory usage\r\n* Current `v2 TRIE` handling still triggers ~600MB memory usage\r\n* Code not yet thread safe, having multiple concurrent streams tied to the same model leads to bad transcriptions.\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.5.0 release\r\n\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Josh Meyer](https:\/\/github.com\/JRMeyer)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [dabinat](https:\/\/github.com\/dabinat)\r\n* [Carlos Fonseca Murillo](https:\/\/github.com\/carlfm01)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Antoine Rey](https:\/\/github.com\/Arey)\r\n* [Jordan Olafsen](https:\/\/github.com\/jorxster)\r\n* [Cem Philipp Freimoser](https:\/\/github.com\/cfreemoser)\r\n* [Nicolas Panel](https:\/\/github.com\/nicolaspanel)\r\n* [Davud Kakaie](https:\/\/github.com\/roxima)\r\n* [Quentin Brunet](https:\/\/github.com\/qboot)\r\n* [Michele Di Giorgio](https:\/\/github.com\/mdigiorgio)\r\n* [Kristian Kankainen](https:\/\/github.com\/kristiank)\r\n* [William Van Woensel](https:\/\/github.com\/william-vw)\r\n* [Igor Fritzsch](https:\/\/github.com\/igorfritzsch)\r\n* [Daniel Winkler](https:\/\/github.com\/danielwinkler)\r\n* [Dan Steinman](https:\/\/github.com\/dsteinman)",
        "78": "",
        "79": "",
        "80": "",
        "81": "",
        "82": "",
        "83": "",
        "84": "",
        "85": "",
        "86": "",
        "87": "",
        "88": "",
        "89": "# General\r\n\r\nThis is the 0.4.1 release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n[v0.4.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.4.1.tar.gz)\r\n\r\nand a trained model\r\n\r\n[deepspeech-0.4.1-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.4.1\/deepspeech-0.4.1-models.tar.gz)\r\n\r\ntrained on American English which achieves an 8.26% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12) (models with \"rounded\" in their file name have rounded weights and those with a \"*.pbmm\" extension are memory mapped and much more memory efficient), and example audio\r\n\r\n[audio-0.4.1.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.4.1\/audio-0.4.1.tar.gz)\r\n\r\nwhich can be used to test the engine and checkpoint files\r\n\r\n[deepspeech-0.4.1-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.4.1\/deepspeech-0.4.1-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n* Removed `initialize_from_frozen_model` flag and support code (46d1cece4f98d726d88579500734a95686cfe16b)\r\n* Updated to TensorFlow r1.12 (26f7bc30f03c424cb918e53dd9decc7a8c81991b, d2438397ce3795f802a864a58e31286db12b3e89)\r\n* Switched to CTC algorithm from TensorFlow's to to [ctcdecode](https:\/\/github.com\/parlance\/ctcdecode\/) (#1679, #1693, #1696)\r\n* Remove old AOT model (1540fa392e4aa64c5a74124a0dd4fd57c993b6f0)\r\n* Introduced NodeJS v11.x support  (5536e5440ee3cf7ea133c003b09f6a3ee8592d71, 4867d6baa444453b0dbd10b308669836496fa0ad)\r\n* Directly exporting TFLite ready model (5d30afdbad77084b090aecbf21bd78c551406671, d6642da05baa2a461ef0943d1ccaa2cdfd91c7ba)\r\n* Added streaming API Support to the GUI Tool (851fb4ea900b17fc2993a83b606879a210ced86e)\r\n* Fixed Fisher + Switchboard importers (9aa23ed387439257579ac5bd9869b43ecd432248, f03669171f1563e3087999d74f26290db10a8ccd, b37f1a7aa5fbc8a3af7b71df5660d36965cad83a, b2f967ac4ddab33fda9255990bbb734f7bbb72f2, 6c903c8b19dc763c3bddfd9f98eb23fe2ebd80af, ba622892aafe957011ab390ae2c6f2d34af088e9, 81b16002b7016e10940c66a7f442fcfea6339244)\r\n* Fixes in Switchboard importer (8507f992c092325c8888680e8b80316e0955c765, e47e344a8272bab0426e380e161383c6f5872a5b, 4efc5c6c4d5b3b3f885492d340165a9e1e00be51)\r\n* Add example for Python streaming from mic with VAD (74cebb83b6da2ba619661f076e80c32a0c651b2f)\r\n* Fixed MFCC window size + stride (1df9602c954ba83e88d61cb8aeb07d8ed1169305, 2a8128b8fd235fb8179b95bd89b84b2d9ba51da2)\r\n* Renamed `LM_WEIGHT` and `VALID_WORD_COUNT_WEIGHT` to `LM_ALPHA` and `LM_BETA` respectively and changed their values (fc46f4382d07c994681a9fc5915ffd5383f596ef)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 TitanX Pascal GPUs (12GB of VRAM).\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora, as well as a pre-release snapshot of the English Common Voice training corpus.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean and other dev corpora, as well as a pre-release snapshot of the English Common Voice validation corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 24\r\n  * `dev_batch_size` 48\r\n  * `test_batch_size` 48\r\n  * `epoch` 30\r\n  * `learning_rate` 0.0001\r\n  * `display_step` 0\r\n  * `validation_step` 1\r\n  * `dropout_rate` 0.15\r\n  * `checkpoint_step` 1\r\n  * `n_hidden` 2048\r\n  * `lm_alpha` 0.75\r\n  * `lm_beta` 1.85\r\n\r\nThe weights with the best validation loss were selected at the end of the 30 epochs.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 2.7, 3.4, 3.5, 3.6 and 3.7) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x, 10.x, and 11.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech) which is installed by following the instructions on the external Go repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.10, 10.11, 10.12, 10.13 and 10.14\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Stretch on Raspberry Pi 3\r\n* ARM64 built against Debian\/ARMbian Stretch and tested on LePotato boards\r\n* Java Android bindings \/ demo app. Early preview, tested only on Pixel 2 device, TF Lite model only\r\n\r\n# Known Issues\r\n\r\n* Feature caching speeds training but increases memory usage\r\n* Current `v2 TRIE` handling still triggers ~600MB memory usage\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.4.1 release\r\n\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Carlos Fonseca Murillo](https:\/\/github.com\/carlfm01)\r\n* [Rob Smith](https:\/\/github.com\/robmsmt\/)\r\n* [Igor Fritzsch](https:\/\/github.com\/igorfritzsch)\r\n* [Josh Meyer](https:\/\/github.com\/JRMeyer)\r\n* [Nicolas Panel](https:\/\/github.com\/nicolaspanel)\r\n* [Benjamin Abel](https:\/\/github.com\/benjaminabel)\r\n* [Vamsi Krishna Surapureddi](https:\/\/github.com\/vamsilnm)\r\n* [b-ak](https:\/\/github.com\/b-ak)\r\n* [David Zurow](https:\/\/github.com\/daanzu)\r\n* [C\u00e9lian Garcia](https:\/\/github.com\/celian-garcia)\r\n* [Jegathesan Shanmugam](https:\/\/github.com\/nullbyte91)\r\n* [Jahir Islam](https:\/\/github.com\/j-a-h-i-r)\r\n* [Jan Engelmohr](https:\/\/github.com\/visi0nary)\r\n* [Mikhail Kuznetcov](https:\/\/github.com\/shershen08)\r\n",
        "90": "",
        "91": "",
        "92": "",
        "93": "# General\r\n\r\nThis is the 0.4.0 release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n[v0.4.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.4.0.tar.gz)\r\n\r\nand a trained model\r\n\r\n[deepspeech-0.4.0-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.4.0\/deepspeech-0.4.0-models.tar.gz)\r\n\r\ntrained on American English which achieves an 8.26% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12) (**The incorrect model was uploaded this will be fixed in 0.4.1**) (models with \"rounded\" in their file name have rounded weights and those with a \"*.pbmm\" extension are memory mapped and much more memory efficient), and example audio\r\n\r\n[audio-0.4.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.4.0\/audio-0.4.0.tar.gz)\r\n\r\nwhich can be used to test the engine and checkpoint files\r\n\r\n[deepspeech-0.4.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.4.0\/deepspeech-0.4.0-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n* Removed `initialize_from_frozen_model` flag and support code (46d1cece4f98d726d88579500734a95686cfe16b)\r\n* Updated to TensorFlow r1.12 (26f7bc30f03c424cb918e53dd9decc7a8c81991b, d2438397ce3795f802a864a58e31286db12b3e89)\r\n* Switched to CTC algorithm from TensorFlow's to to [ctcdecode](https:\/\/github.com\/parlance\/ctcdecode\/) (#1679, #1693, #1696)\r\n* Remove old AOT model (1540fa392e4aa64c5a74124a0dd4fd57c993b6f0)\r\n* Introduced NodeJS v11.x support  (5536e5440ee3cf7ea133c003b09f6a3ee8592d71, 4867d6baa444453b0dbd10b308669836496fa0ad)\r\n* Directly exporting TFLite ready model (5d30afdbad77084b090aecbf21bd78c551406671, d6642da05baa2a461ef0943d1ccaa2cdfd91c7ba)\r\n* Added streaming API Support to the GUI Tool (851fb4ea900b17fc2993a83b606879a210ced86e)\r\n* Fixed Fisher + Switchboard importers (9aa23ed387439257579ac5bd9869b43ecd432248, f03669171f1563e3087999d74f26290db10a8ccd, b37f1a7aa5fbc8a3af7b71df5660d36965cad83a, b2f967ac4ddab33fda9255990bbb734f7bbb72f2, 6c903c8b19dc763c3bddfd9f98eb23fe2ebd80af, ba622892aafe957011ab390ae2c6f2d34af088e9, 81b16002b7016e10940c66a7f442fcfea6339244)\r\n* Fixes in Switchboard importer (8507f992c092325c8888680e8b80316e0955c765, e47e344a8272bab0426e380e161383c6f5872a5b, 4efc5c6c4d5b3b3f885492d340165a9e1e00be51)\r\n* Add example for Python streaming from mic with VAD (74cebb83b6da2ba619661f076e80c32a0c651b2f)\r\n* Fixed MFCC window size + stride (1df9602c954ba83e88d61cb8aeb07d8ed1169305, 2a8128b8fd235fb8179b95bd89b84b2d9ba51da2)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 TitanX Pascal GPUs (12GB of VRAM).\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora, as well as a pre-release snapshot of the English Common Voice training corpus.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean and other dev corpora, as well as a pre-release snapshot of the English Common Voice validation corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 24\r\n  * `dev_batch_size` 48\r\n  * `test_batch_size` 48\r\n  * `epoch` 30\r\n  * `learning_rate` 0.0001\r\n  * `display_step` 0\r\n  * `validation_step` 1\r\n  * `dropout_rate` 0.15\r\n  * `checkpoint_step` 1\r\n  * `n_hidden` 2048\r\n  * `lm_alpha` 0.75\r\n  * `lm_beta` 1.85\r\n\r\nThe weights with the best validation loss were selected at the end of the 30 epochs.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 2.7, 3.4, 3.5, 3.6 and 3.7) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x, 10.x, and 11.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n* [Go](https:\/\/github.com\/asticode\/go-astideepspeech)\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.10, 10.11, 10.12, 10.13 and 10.14\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Stretch on Raspberry Pi 3\r\n* ARM64 built against Debian\/ARMbian Stretch and tested on LePotato boards\r\n\r\n\r\n# Known Issues\r\n\r\n* Feature caching speeds training but increases memory usage\r\n* Current `v2 TRIE` handling still triggers ~600MB memory usage\r\n* Incorrect model was uploaded to release which will be fixed in 0.4.1\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.4.0 release\r\n\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Carlos Fonseca Murillo](https:\/\/github.com\/carlfm01)\r\n* [Rob Smith](https:\/\/github.com\/robmsmt\/)\r\n* [Igor Fritzsch](https:\/\/github.com\/igorfritzsch)\r\n* [Josh Meyer](https:\/\/github.com\/JRMeyer)\r\n* [Nicolas Panel](https:\/\/github.com\/nicolaspanel)\r\n* [Benjamin Abel](https:\/\/github.com\/benjaminabel)\r\n* [Vamsi Krishna Surapureddi](https:\/\/github.com\/vamsilnm)\r\n* [b-ak](https:\/\/github.com\/b-ak)\r\n* [David Zurow](https:\/\/github.com\/daanzu)\r\n* [C\u00e9lian Garcia](https:\/\/github.com\/celian-garcia)\r\n* [Jegathesan Shanmugam](https:\/\/github.com\/nullbyte91)\r\n* [Jahir Islam](https:\/\/github.com\/j-a-h-i-r)\r\n* [Jan Engelmohr](https:\/\/github.com\/visi0nary)\r\n* [Mikhail Kuznetcov](https:\/\/github.com\/shershen08)\r\n",
        "94": "",
        "95": "",
        "96": "",
        "97": "# General\r\n\r\nThis is the 0.3.0 release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n[v0.3.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.3.0.tar.gz)\r\n\r\nand a trained model\r\n\r\n[deepspeech-0.3.0-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.3.0\/deepspeech-0.3.0-models.tar.gz)\r\n\r\ntrained on American English which achieves an 11% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12) (models with \"rounded\" in their file name have rounded weights and those with a \"*.pbmm\" extension are memory mapped and much more memory efficient), and example audio\r\n\r\n[audio-0.3.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.3.0\/audio-0.3.0.tar.gz)\r\n\r\nwhich can be used to test the engine and checkpoint files\r\n\r\n[deepspeech-0.3.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.3.0\/deepspeech-0.3.0-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n* Updated to Tensorflow 1.11 (f3dacc8ce90bef2fd99dfef77c9cb88fea3af38e, 8a5a0009c1772928a180ae48bd90e19acc2049e9, a0a9415ee98d786b6c3b6c65ac488f02d0257fb6, 1385cf6f9b7333f835053e27e6825db26b2be3b4, 72eaa2e8cea0c4c4412f8cd10c17b78a0bb3ae95)\r\n* Fixed indeterminacy when using the language model (63529387e3b70d06a7fb304dec83a4ae6393a052)\r\n* Fixed memory leak (6e67dcdbfc6cbf4d449d322b3ba1ad4521f31d29)\r\n* Changed trie file format to help save some loading time (7c5e031b1f5863da158e864bbb35383bb20130c7, 5465aa0176d59a5db6df3c2cf4799592e3d4644f)\r\n* Added command line tool + GUI for transcribing audio clips (de54619ac59b1310c96dc548cc21fe4ffb6a03d1)\r\n* Removed ahead-of-time AOT compilation (4b66535d2ac2616bc237e7a8bda01105a33fb9ef)\r\n* Fixed errors with uninitialized update_progressbar field (e746c50c28e88731a249d6604cea5388a1aaf167)\r\n* Fixed evaluation and single inference code (e00c63c0a8028d32bae30e68bc461478f486e535, 1b8b46ec9570e073ea5686baede615c0c8854b49)\r\n* Moved to GCC 7.2 on ARMv7 and Aarch64 (a093848d16c83278d345ec56566674523dee6da1)\r\n* Remove deprecated tensorflow.contrib.learn.python.learn.datasets.base.maybe_download (94b8c5c1a3efefc0049264eae62ed4b9e6d2d503)\r\n* Adding new importer for https:\/\/github.com\/nicolaspanel\/TrainingSpeech (baf962471493da2742dd15eafda8d4b6c9073936)\r\n* Fixed regression breaking Common Voice importer (529ebdfcb4507c7ced8f42b2d386b8a780a39551)\r\n* Fixed improper doc strings (06d4379994452e7eb72c839508f185b2ebc32b86)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 TitanX Pascal GPUs (12GB of VRAM).\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora, as well as a pre-release snapshot of the English Common Voice training corpus.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean and other dev corpora, as well as a pre-release snapshot of the English Common Voice validation corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 24\r\n  * `dev_batch_size` 48\r\n  * `test_batch_size` 48\r\n  * `epoch` 30\r\n  * `learning_rate` 0.0001\r\n  * `display_step` 0\r\n  * `validation_step` 1\r\n  * `dropout_rate` 0.2\r\n  * `checkpoint_step` 1\r\n  * `n_hidden` 2048\r\n\r\nThe weights with the best validation loss were selected at the end of the 30 epochs.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 2.7, 3.4, 3.5, 3.6 and 3.7) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x and 10.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.10, 10.11, 10.12, 10.13 and 10.14\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Stretch on Raspberry Pi 3\r\n* ARM64 built against Debian\/ARMbian Stretch and tested on LePotato boards\r\n\r\n\r\n# Known Issues\r\n\r\n* Feature caching speeds training but increases memory usage\r\n* Current `v2 TRIE` handling still triggers ~600MB memory usage\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.3.0 release\r\n\r\n* [Bhargav](https:\/\/github.com\/b-ak)\r\n* [Faissal Bensefia](https:\/\/github.com\/faissaloo)\r\n* [daanzu](https:\/\/github.com\/daanzu)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Nicolas Panel](https:\/\/github.com\/nicolaspanel)\r\n",
        "98": "",
        "99": "",
        "100": "",
        "101": "",
        "102": "# General\r\n\r\nThis is the 0.2.0 release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n[v0.2.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/archive\/v0.2.0.tar.gz)\r\n\r\nand a trained model\r\n\r\n[deepspeech-0.2.0-models.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.2.0\/deepspeech-0.2.0-models.tar.gz)\r\n\r\ntrained on American English which achieves an 11% word error rate on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12) (models with \"rounded\" in their file name have rounded weights and those with a \"*.pbmm\" extension are memory mapped and much more memory efficient), and example audio\r\n\r\n[audio-0.2.0.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.2.0\/audio-0.2.0.tar.gz)\r\n\r\nwhich can be used to test the engine and checkpoint files\r\n\r\n[deepspeech-0.2.0-checkpoint.tar.gz](https:\/\/github.com\/mozilla\/DeepSpeech\/releases\/download\/v0.2.0\/deepspeech-0.2.0-checkpoint.tar.gz)\r\n\r\nwhich can be used as the basis for further fine-tuning.\r\n\r\n# Notable changes from the previous release\r\n\r\n* Made Deep Speech streamable, i.e. able to do inference while audio is streaming in (#1463)\r\n* Introduced new streaming API, example usage in this [gist](https:\/\/gist.github.com\/reuben\/80d64de15d1f46d34d28c7e83fc5f57e#file-ds_mic-py) (#1463)\r\n* Added feature caching, precomputing + caching audio features to speed training (#1532)\r\n* Added progressbar to indicate training progress (#1488)\r\n* Updated Dockerfile's cuDNN version from 7.1.1 to 7.2.1 (1a7ac22)\r\n* Removed old training + website scripts (#1539)\r\n* Pre-built binaries now work with upstream TensorFlow 1.6 (c579b74)\r\n* Switched to [LSTMBlockFusedCell](https:\/\/www.tensorflow.org\/versions\/r1.6\/api_docs\/python\/tf\/contrib\/rnn\/LSTMBlockFusedCell) (0b95ed6)\r\n* Added tool to convert graph protobuf to pbtxt (4e383ac)\r\n* Added tool to find out which ops are needed by a graph (d2be00f)\r\n* Added Non-positional arguments everywhere (646c917)\r\n* Added support for Node.JS 10 (#1396)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a server with 8 TitanX Pascal GPUs (12GB of VRAM).\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora, as well as a pre-release snapshot of the English Common Voice training corpus.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean and other dev corpora, as well as a pre-release snapshot of the English Common Voice validation corpus.\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean test corpus\r\n  * `train_batch_size` 24\r\n  * `dev_batch_size` 48\r\n  * `test_batch_size` 48\r\n  * `epoch` 30\r\n  * `learning_rate` 0.0001\r\n  * `display_step` 0\r\n  * `validation_step` 1\r\n  * `dropout_rate` 0.2\r\n  * `checkpoint_step` 1\r\n  * `n_hidden` 2048\r\n\r\nThe weights with the best validation loss were selected at the end of the 30 epochs.\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 2.7, 3.4, 3.5, 3.6 and 3.7) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x, 9.x and 10.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.10, 10.11, 10.12, 10.13 and 10.14\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Stretch on Raspberry Pi 3\r\n* ARM64 built against Debian\/ARMbian Stretch and tested on LePotato boards\r\n\r\n\r\n# Known Issues\r\n\r\n* Feature caching speeds training but increases memory usage\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.2.0 release\r\n\r\n* [Aaron Blew](https:\/\/github.com\/blewa)\r\n* [Aaron Owen](https:\/\/github.com\/aaron-owen)\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Bhavani Subramanian](https:\/\/github.com\/bhavani-subramanian)\r\n* [Divyansh Jha](https:\/\/github.com\/divyanshj16)\r\n* [est31](https:\/\/github.com\/est31)\r\n* [Francis Tyers](https:\/\/github.com\/ftyers)\r\n* [George Fedoseev](https:\/\/github.com\/GeorgeFedoseev)\r\n* [Godeffroy](https:\/\/github.com\/godefv)\r\n* [Guilherme Nardari](https:\/\/github.com\/gnardari)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Mike Sheldon](https:\/\/github.com\/Elleo)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [Sam Davis](https:\/\/github.com\/samgd)\r\n* [Scott Stevenson](https:\/\/github.com\/srstevenson)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [the-nose-knows](https:\/\/github.com\/the-nose-knows)",
        "103": "# General\r\n\r\nThis is the 0.1.1 release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n`v0.1.1.tar.gz`\r\n\r\nand a model, not yet optimized for size,\r\n\r\n`deepspeech-0.1.1-models.tar.gz`\r\n\r\ntrained on American English which achieves a ~~*5.6% word error rate*~~ (The language model included some test data.) on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12), and example audio\r\n\r\n`audio-0.1.1.tar.gz`\r\n\r\nwhich can be used to test the engine and checkpoint files\r\n\r\n`deepspeech-0.1.1-checkpoint.tar.gz`\r\n\r\nwhich can be used as the basis for further fine-tuning. Unfortunately licensing issues prevent us from releasing the text used to train the language model.\r\n\r\n# Notable changes from the previous release\r\n\r\n* [Rust bindings](https:\/\/github.com\/RustAudio\/deepspeech-rs) were contributed by [RustAudio](https:\/\/github.com\/RustAudio)\r\n* Lowering dependency on AVX2 to AVX instruction sets (mozilla\/tensorflow#46)\r\n* Pre-built binaries now work with upstream TensorFlow 1.4 (mozilla\/tensorflow#43)\r\n* Switching GPU build to CUDA 8.0 \/ CuDNN v6 (mozilla\/tensorflow#43)\r\n* Added support for Node.JS 7\/8\/9 (#1042)\r\n* Initializing a training run from a frozen graph (eg. a release model) is now easier (#1149)\r\n* The Python package no longer holds the GIL during inference and can be used in multi-threaded Python programs (#1164)\r\n* The Python package now works on macOS 10.10 and 10.11 (#1065)\r\n\r\n# Hyperparameters for fine-tuning\r\n\r\nThe hyperparameters used to train the model are useful for fine tuning. Thus, we document them here along with the hardware used, a two node cluster where each node has 8 TitanX Pascal GPU's.\r\n\r\n  * `train_files` [Fisher](https:\/\/pdfs.semanticscholar.org\/a723\/97679079439b075de815553c7b687ccfa886.pdf), [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf), and [Switchboard](http:\/\/ieeexplore.ieee.org\/document\/225858\/) training corpora.\r\n  * `dev_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean  dev corpus\r\n  * `test_files` [LibriSpeech](http:\/\/www.danielpovey.com\/files\/2015_icassp_librispeech.pdf) clean  test corpus\r\n  * `train_batch_size` 12\r\n  * `dev_batch_size` 8\r\n  * `test_batch_size` 8\r\n  * `epoch` 13\r\n  * `learning_rate` 0.0001\r\n  * `display_step` 0\r\n  * `validation_step` 1\r\n  * `dropout_rate` 0.2367\r\n  * `default_stddev` 0.046875\r\n  * `checkpoint_step` 1\r\n  * `log_level` 0\r\n  * `checkpoint_dir` value specific to hardware setup\r\n  * `wer_log_pattern` \"GLOBAL LOG: logwer('${COMPUTE_ID}', '%s', '%s', %f)\"\r\n  * `decoder_library_path` value specific to hardware setup\r\n  * `n_hidden` 2048\r\n\r\n# Bindings\r\n\r\nThis release also includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 2.7, 3.4, 3.5, and 3.6) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, 6.x, 7.x, 8.x and 9.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.10, 10.11, 10.12 and 10.13\r\n* Linux x86 64 bit with a modern CPU (Needs at least AVX\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Jessie on Raspberry Pi 3\r\n\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base.\r\n\r\n# Contributors to 0.1.1 release\r\n\r\n* [Alexandre Lissy](https:\/\/github.com\/lissyx)\r\n* [Kelly Davis](https:\/\/github.com\/kdavis-mozilla)\r\n* [Reuben Morais](https:\/\/github.com\/reuben)\r\n* [qin](https:\/\/github.com\/qin)\r\n* [tmm2018](https:\/\/github.com\/tmm2018)\r\n* [Nobu Funaki](https:\/\/github.com\/nobuf)\r\n* [dwks](https:\/\/github.com\/dwks)\r\n* [Mikalai Drabovich](https:\/\/github.com\/MikalaiDrabovich)\r\n* [Tilman Kamp](https:\/\/github.com\/tilmankamp)\r\n* [lparam](https:\/\/github.com\/lparam)\r\n* [Steffen Schneider](https:\/\/github.com\/stes)\r\n* [wgent](https:\/\/github.com\/wgent)\r\n* [Chandan Rai](https:\/\/github.com\/crowchirp)\r\n* [Jan](https:\/\/github.com\/JanX2)\r\n* [Michael Henretty](https:\/\/github.com\/mikehenrty)\r\n* [Piyush Kumar](https:\/\/github.com\/pikumar)\r\n* [Quentin Renard](https:\/\/github.com\/asticode)\r\n* [Sergey Gonimar](https:\/\/github.com\/gonimar)\r\n* [Sriram Velamur](https:\/\/github.com\/techiev2)\r\n* [farwayer](https:\/\/github.com\/farwayer)\r\n* [interfect](https:\/\/github.com\/interfect)",
        "104": "# General\r\n\r\nThis is the initial release of __Deep Speech__, an open speech-to-text engine. This release includes source code\r\n\r\n`v0.1.0.tar.gz`\r\n\r\nand a model, not yet optimized for size,\r\n\r\n`deepspeech-0.1.0-models.tar.gz`\r\n\r\ntrained on American English which achieves a ~~*6.0% word error rate*~~ (The language model included some test data.) on the [LibriSpeech clean test corpus](http:\/\/www.openslr.org\/12), and example audio\r\n\r\n`audio-0.1.0.tar.gz`\r\n\r\nwhich can be used to test the engine.\r\n\r\n# Bindings\r\n\r\nIn addition it includes a Python based command line tool `deepspeech`, installed through\r\n```\r\npip install deepspeech\r\n```\r\nAlternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n```bash\r\npip install deepspeech-gpu\r\n```\r\n\r\nAlso, it exposes bindings for the following languages\r\n\r\n* [Python](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-python-package) (Versions 2.7, 3.4, 3.5, and 3.6) installed via\r\n  ```bash\r\n  pip install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```bash\r\n  pip install deepspeech-gpu\r\n  ```\r\n* [NodeJS](https:\/\/github.com\/mozilla\/DeepSpeech#using-the-nodejs-package) (Versions 4.x, 5.x, and 6.x) installed via\r\n  ```\r\n  npm install deepspeech\r\n  ```\r\n  Alternatively, quicker inference can be performed using a supported NVIDIA GPU on Linux. (See below to find which GPU's are supported.) This is done by instead installing the GPU specific package:\r\n  ```\r\n  npm install deepspeech-gpu\r\n  ```\r\n* [C++](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/native_client\/client.cc) which requires the appropriate shared objects are installed from `native_client.tar.xz` (See the section in the main [README](https:\/\/github.com\/mozilla\/DeepSpeech\/blob\/master\/README.md#using-the-command-line-client) which describes `native_client.tar.xz` installation.)\r\n\r\nIn addition there are third party bindings that are supported by external developers, for example\r\n\r\n* [Rust](https:\/\/github.com\/RustAudio\/deepspeech-rs) which is installed by following the instructions on the external Rust repo.\r\n\r\n\r\n# Supported Platforms\r\n\r\n* OS X 10.12 and 10.13\r\n* Linux x86 64 bit with a modern CPU (Supports up to AVX2\/FMA)\r\n* Linux x86 64 bit with a modern CPU + NVIDIA GPU (Compute Capability at least 3.0, see [NVIDIA docs](https:\/\/developer.nvidia.com\/cuda-gpus))\r\n* Raspbian Jessie on Raspberry Pi 3\r\n\r\n\r\n# Contact\/Getting Help\r\n\r\n1. [FAQ](https:\/\/github.com\/mozilla\/DeepSpeech\/wiki#frequently-asked-questions) - We have a list of common questions, and their answers, in our FAQ. When just getting started, it's best to first check the FAQ to see if your question is addressed.\r\n2. [Discourse Forums](https:\/\/discourse.mozilla.org\/c\/deep-speech) - If your question is not addressed in the FAQ, the Discourse Forums is the next place to look. They contain conversations on [General Topics](https:\/\/discourse.mozilla.org\/t\/general-topics), [Using Deep Speech](https:\/\/discourse.mozilla.org\/t\/using-deep-speech), [Alternative Platforms](https:\/\/discourse.mozilla.org\/t\/alternative-platforms), and [Deep Speech Development](https:\/\/discourse.mozilla.org\/t\/deep-speech-development).\r\n3. [IRC](https:\/\/wiki.mozilla.org\/IRC) - If your question is not addressed by either the FAQ or Discourse Forums, you can contact us on the `#machinelearning` channel on Mozilla IRC; people there can try to answer\/help\r\n4. [Issues](https:\/\/github.com\/mozilla\/deepspeech\/issues) - Finally, if all else fails, you can open an issue in our repo if there is a bug with the current code base."
    }
}