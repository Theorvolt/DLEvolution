{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from tools import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(environ['user'],environ['token'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Commits\n",
    "\n",
    "We can retrieve the patch/diff information of a commit. Note that this is one commit call, and Github is limited to 5000 requests an hour, i.e 5000 commits scraped. Let's sample some from SpeechBrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_list = list(map(lambda x:x['sha'],flatten([g.get_commits('speechbrain/speechbrain',{'page':x+1,'per_page':100}).json() for x in range(2)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@@ -18,7 +18,7 @@ Here is a list of the different languages that we tested within the CommonVoice\\n | Language | CommonVoice Release | hyperparams file | LM | Val. CER | Val. WER | Test CER | Test WER | HuggingFace link | Model link | GPUs |\\n | ------------- |:-------------:|:---------------------------:| -----:| -----:| -----:| -----:| -----:| :-----------:| :-----------:| :-----------:|\\n | English | 2020-12-11 | train_en_with_wav2vec.yaml | No | 5.01 | 12.57 | 7.32 | 15.58 | Not Avail. | [model](https://drive.google.com/drive/folders/1tYO__An68xrM5pR1UIXzEkwzvKX2Tz2o?usp=sharing) | 2xV100 32GB |\\n-| French | 2020-12-11 | train_fr_with_wav2vec.yaml | No | 2.60 | 8.59 | 3.19 | 9.96 | Not Avail. | [model](https://drive.google.com/drive/folders/1T9DfdZwcNI9CURxhLCi8GA5JVz8adiY8?usp=sharing) | 2xV100 32GB |\\n+| French | 2020-12-11 | train_fr_with_wav2vec.yaml | No | 2.60 | 8.59 | 3.19 | 9.96 | [model](https://huggingface.co/speechbrain/asr-wav2vec2-commonvoice-fr) | [model](https://drive.google.com/drive/folders/1T9DfdZwcNI9CURxhLCi8GA5JVz8adiY8?usp=sharing) | 2xV100 32GB |\\n | Italian | 2020-12-11 | train_it_with_wav2vec.yaml | No | 2.77 | 9.83 | 3.16 | 10.85 | Not Avail. | [model](https://drive.google.com/drive/folders/1JhlxeA04tWg_vKcNChOoXSnjBe4luRby?usp=sharing) | 2xV100 32GB |\\n | Kinyarwanda | 2020-12-11 | train_rw_with_wav2vec.yaml | No | 6.20 | 20.07 | 8.25 | 23.12 | Not Avail. | [model](https://drive.google.com/drive/folders/12_BDenvOqEERDZLAN-KdiAHklvuo35tx?usp=sharing) | 2xV100 32GB |\\n '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.get_commit_diff('speechbrain/speechbrain',commit_list[0]).json()['files'][0]['patch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter words\n",
    "\n",
    "I have inputted some PyTorch layers to pick up in the process of reading line diffs. This is to be expanded for things like hyperparameters and eventually grouping by Layer/Hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words = ['Conv1d','Conv2d','Conv3d', 'ConvTranspose1d','ConvTranspose1d','ConvTranspose2d','ConvTranspose3d','LazyConv1d','LazyConv2d','LazyConv3d','LazyConvTranspose1d','LazyConvTranspose2d','LazyConvTranspose3d','Unfold','Fold',\n",
    "'MaxPool1d', 'MaxPool2d', 'MaxPoool3d', 'MaxUnpool1d', 'MaxUnpool2d','MaxUnpool3d', 'AvgPool1d', 'AvgPool2d','AvgPool3d', 'LPPool1d','LPPool2d', 'LPPool3d', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AdaptiveAvgPool1d',\n",
    "'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d',\n",
    "'ReflectionPad1d', 'ReflectionPad2d', 'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'ZeroPad2d', 'ConstantPad1d', 'ConstantPad2d', 'ConstantPad3d',\n",
    "'ELU', 'Hardshrink', 'Hardsigmoid', 'Hardtanh', 'Hardswish', 'LeakyReLU', 'LogSigmoid', 'MultiheadAttention', 'PReLU', 'ReLU', 'RReLU', 'SELU', 'CELU', 'GELU', 'Sigmoid', 'SiLU', 'Mish', 'Softplus', 'Softshrink', 'Softsign', 'Tanh', 'Tanhshrink', 'Threshold',\n",
    "'Softmin', 'Softmax','Softmax2d', 'Softmax2d', 'LogSoftmax', 'AdaptiveLogSoftmaxWithLoss',\n",
    "'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d', 'GroupNorm', 'InstanceNorm1d', 'InstanceNorm2d', 'InstanceNorm3d', 'LayerNorm',\n",
    "'RNN', \"LSTM\", 'GRU', 'RNNCell', 'LSTMCell', 'GRUCell',\n",
    "'Transformer', 'TransformerEncoder', 'TransformerDecoder', 'Linear', 'Bilinear', 'LazyLinear', 'Identity',\n",
    "'Dropout', 'Dropout2d', 'Dropout3d', 'AlphaDropout',\n",
    "'Embedding',\n",
    "'batch_size', 'num_epochs', 'epochs', 'n_hidden'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for commit_sha in commit_list[:200]:\n",
    "    try:\n",
    "        commit = g.get_commit_diff('speechbrain/speechbrain',commit_sha).json()\n",
    "        com = commit['files']\n",
    "    except KeyError: \n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        commit_history.append([commit['sha'],[\n",
    "        [file['filename'],[\n",
    "        [line[0],set(re.findall('|'.join(filter_words), line))] for line in \n",
    "        [line  for line in file['patch'].split('\\n') if  len(line) > 2 and ((line[0]=='+' and line[1] != '+') or (line[0]=='-' and line[1] != '-'))] \n",
    "        if len(set(re.findall('|'.join(filter_words), line))) != 0]\n",
    "        ] \n",
    "        for file in com if file['filename'].split('.')[-1] == 'py']])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['f968c46fe6e4eb07202e32ca54335112acc42195', []],\n",
       " ['6d44f957bad493bba63ffaac86201aa1f31f250c', [['speechbrain/core.py', []]]],\n",
       " ['b279e6fc038bbaf1ed0d5c5cdbb2636ee5806bba', [['speechbrain/core.py', []]]],\n",
       " ['8030182de32c3188e1a4103fbcd49be9f3b486b2',\n",
       "  [['speechbrain/utils/metric_stats.py', []],\n",
       "   ['tests/unittests/test_metrics.py', []]]],\n",
       " ['e14d2c15874a22df018edf441af62b105b33ed39',\n",
       "  [['speechbrain/core.py',\n",
       "    [['+', {'epochs'}], ['-', {'epochs'}], ['+', {'epochs'}]]]]],\n",
       " ['5cab96e21a1e793c8b294de21bc5e6d897e3e62b',\n",
       "  [['speechbrain/utils/metric_stats.py', []]]],\n",
       " ['8b914ff01099ff879cebeabdd18cc207561a55ae',\n",
       "  [['speechbrain/utils/metric_stats.py', []]]],\n",
       " ['e1b7ea21be4a6b711b6bd410f6cfb20427d82d03',\n",
       "  [['speechbrain/core.py', [['-', {'epochs'}], ['+', {'epochs'}]]]]],\n",
       " ['3e4d85ae6f558742f4760bf328230086e54fccc1',\n",
       "  [['speechbrain/core.py',\n",
       "    [['+', {'epochs'}], ['-', {'epochs'}], ['+', {'epochs'}]]]]],\n",
       " ['01b84a7ef4c90cfffc1111e709c140080e630d1a',\n",
       "  [['tests/unittests/test_metrics.py', []]]],\n",
       " ['02b2d2fca4a237a46799005e2cd13a8216a7cb4e',\n",
       "  [['speechbrain/utils/metric_stats.py', []]]],\n",
       " ['7d93a4124113a9d7b17787c3747883449f00eaff', []],\n",
       " ['cf97773a81253ff87315d5b698adcdaf2c592208',\n",
       "  [['speechbrain/utils/metric_stats.py', []],\n",
       "   ['tests/unittests/test_metrics.py', []]]],\n",
       " ['ad0fe4a7fe722604ccd06ecaa2678791689a7411',\n",
       "  [['speechbrain/utils/metric_stats.py', []],\n",
       "   ['tests/unittests/test_metrics.py', []]]],\n",
       " ['51373a57aedf3d58a0b08891e05af7d4bbd987db',\n",
       "  [['speechbrain/utils/metric_stats.py', []]]],\n",
       " ['4aa8782f2062979d9f2344ebf1cea00ee5196bc6', []],\n",
       " ['096a0708509c7064397da51a7298feac707bad19', []],\n",
       " ['c0e398a1fe62700200d3130484d4173be8d2d827',\n",
       "  [['docs/conf.py', []],\n",
       "   ['speechbrain/lobes/models/fairseq_wav2vec.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['fe6e22c052d19b435befc2998b51b72aa17b8ee5', []],\n",
       " ['b0eb2517884056902b9cffa6dcdb760b2c10dc36', []],\n",
       " ['104bb159e9acb9c31b8c61530291af567a11f5b4', [['docs/conf.py', []]]],\n",
       " ['806471973490a14277091b787797b675f5931083', []],\n",
       " ['2a0a6c80c45d46f527acd0da5e08d5ac687d3ed0',\n",
       "  [['docs/conf.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['6a01ea2ecd556c86806aa4dc90eb311c58356000',\n",
       "  [['docs/conf.py', []],\n",
       "   ['speechbrain/lobes/models/fairseq_wav2vec.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['0d57cfc6499deda9688d6444518a4eefb2abe7d8', []],\n",
       " ['78724272057477110f73eaa6f16b43c13d2bf000', []],\n",
       " ['0bebe22ff58771c8423cdfdb6eebfbb626dc4cc2', []],\n",
       " ['dd2f30e737412083af9d7e01f45df272c1e9bfbf',\n",
       "  [['recipes/LibriSpeech/LM/dataset.py', []]]],\n",
       " ['307109145636f5ddf1643082c4feff004f0f3b11',\n",
       "  [['recipes/LibriSpeech/LM/dataset.py', []]]],\n",
       " ['062a17f32c6d67391e73be97b926ec3da3b12a6c',\n",
       "  [['recipes/LibriSpeech/LM/dataset.py', []]]],\n",
       " ['db2cfa5803bd2c31cdc979f424b033117143948a', []],\n",
       " ['03d0226e5ac66f19457dbc1f2272e0b183f39102', []],\n",
       " ['47e8badee325df4d496f972ee6b45d190116380f', []],\n",
       " ['c3e995b358f860724544b34d7a7c7dd35e3cffd2', []],\n",
       " ['d6bfe138a90dff3490f7196acbd9c62939289d46', []],\n",
       " ['1205cb9c4b755dd541218fdb4b71707069a13bbd', []],\n",
       " ['c26aa34e889c65068b98eb4f4ecfc9be65d77735', []],\n",
       " ['8517e9ac119f638564c36fbb1c305d1aab7bddf2',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []],\n",
       "   ['tests/unittests/test_features.py', []],\n",
       "   ['tests/unittests/test_losses.py', []]]],\n",
       " ['f70b6f8b317d4f1207d094deefe526004cd2ee1b', []],\n",
       " ['86e2473b2df7871ff341823207c905888444b2c8',\n",
       "  [['recipes/SLURP/direct/train_with_wav2vec2.py', []]]],\n",
       " ['b50eac982ba495116725e5727964e6a52df2ee49', []],\n",
       " ['8cb958c65cd79ecfa681d7b61b56f39fd78737d1',\n",
       "  [['recipes/CommonVoice/ASR/transducer/train.py', []],\n",
       "   ['recipes/LibriSpeech/ASR/transducer/train.py', []],\n",
       "   ['recipes/TIMIT/ASR/transducer/train.py', []],\n",
       "   ['recipes/TIMIT/ASR/transducer/train_wav2vec.py', []],\n",
       "   ['speechbrain/nnet/losses.py', []]]],\n",
       " ['6f2bf10e8aef20af59621fbbbf2c7db2eb94aa0d', []],\n",
       " ['06fa78a4891624a67df16d7137b6af167c664025',\n",
       "  [['speechbrain/dataio/dataloader.py', []]]],\n",
       " ['9a42142f69c1942a740e0e4e284cb1a5519d4c91',\n",
       "  [['recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py', []]]],\n",
       " ['b9b06edccce34e869679b00566756bfa8445ccaf', []],\n",
       " ['41ceaba5914fe5571efa751b990cbe70ace3448a',\n",
       "  [['recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py', []]]],\n",
       " ['173cd7d0ea78268b02ecb9b43251352191b271a5',\n",
       "  [['speechbrain/nnet/losses.py', []]]],\n",
       " ['0fba8395ec45e5163c83c8665e32b62972720a03', []],\n",
       " ['23fad46eefbfa7ed7ebedf303a0078b555837b98',\n",
       "  [['speechbrain/utils/callchains.py', []],\n",
       "   ['speechbrain/utils/checkpoints.py', []]]],\n",
       " ['bd37735460a6bc8502e29d6fe5da7ff99663a580',\n",
       "  [['recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py', []],\n",
       "   ['recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py', []],\n",
       "   ['recipes/CommonVoice/self-supervised-learning/wav2vec2/common_voice_prepare.py',\n",
       "    []],\n",
       "   ['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py',\n",
       "    [['+', {'Transformer'}]]],\n",
       "   ['recipes/LibriSpeech/ASR/seq2seq/train.py', []],\n",
       "   ['recipes/TIMIT/ASR/seq2seq/train.py', []],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/dataio/sampler.py',\n",
       "    [['-', {'batch_size'}], ['-', {'batch_size'}], ['+', {'batch_size'}]]],\n",
       "   ['speechbrain/decoders/seq2seq.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'Transformer'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]],\n",
       "   ['speechbrain/nnet/normalization.py',\n",
       "    [['+', {'GroupNorm'}], ['+', {'GroupNorm'}], ['+', {'GroupNorm'}]]],\n",
       "   ['tests/unittests/test_normalization.py',\n",
       "    [['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}]]]]],\n",
       " ['398de0865923bebbdccaf168d19b177d97424062',\n",
       "  [['speechbrain/utils/callchains.py', []],\n",
       "   ['speechbrain/utils/checkpoints.py', []]]],\n",
       " ['292518bcf699820fd2d0f1b2c2b874e109051aaa', []],\n",
       " ['4aaabfda07b4d42e85f5d97184d375ef574eff6d',\n",
       "  [['recipes/LibriSpeech/ASR/seq2seq/train.py', []],\n",
       "   ['recipes/TIMIT/ASR/seq2seq/train.py', []],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/dataio/sampler.py',\n",
       "    [['-', {'batch_size'}], ['-', {'batch_size'}], ['+', {'batch_size'}]]]]],\n",
       " ['f53793f37cab066a193e1148ae09d7c4c59c9d37',\n",
       "  [['speechbrain/decoders/seq2seq.py', []]]],\n",
       " ['86247dcbc88e93809ee0dd93db12156fcbc48cd5',\n",
       "  [['recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py', []],\n",
       "   ['recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['9f87ff2d1255f903d143b8ed3bb3138dffff85ca', []],\n",
       " ['d0680c69818429c9a199e67d20abae5fecc6d456',\n",
       "  [['speechbrain/nnet/CNN.py', []]]],\n",
       " ['8be13cb90c593f37883c5a83c67afeb03d38fd3c',\n",
       "  [['speechbrain/nnet/CNN.py', []]]],\n",
       " ['5b17fe755e060bd555c9a9f238cc3c3b5c85edbf',\n",
       "  [['recipes/CommonLanguage/lang_id/train.py', []],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/lobes/models/Xvector.py', []],\n",
       "   ['speechbrain/nnet/CNN.py', []]]],\n",
       " ['ae942228760c6487bba9abf4ae7dc51015314173',\n",
       "  [['recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py', []]]],\n",
       " ['ceb9a8c463fdbbcb2928bb72c64f81e0fef270d6', []],\n",
       " ['0df0dc879edc8b3b46c0975de240d16e8e490d88',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['76907cd54cf93bc116fd9313d4cda63fa26189e8',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['bc64abe6f1a3229e4c16b48a8d911bb8f7fccded',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['e24d73cce3172d6435985cbfef32437a376f2f60',\n",
       "  [['recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py', []]]],\n",
       " ['32149120c1132e2a3c25522023318e4bc1d17152', []],\n",
       " ['e304919dc615cbb0b477b58e6bd80cca1b37d2d8',\n",
       "  [['recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py', []],\n",
       "   ['recipes/CommonVoice/self-supervised-learning/wav2vec2/common_voice_prepare.py',\n",
       "    []],\n",
       "   ['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py',\n",
       "    [['+', {'Transformer'}]]],\n",
       "   ['recipes/self-supervised-learning/wav2vec2/common_voice_prepare.py', []],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'batch_size'}], ['+', {'batch_size'}]]],\n",
       "   ['speechbrain/nnet/normalization.py',\n",
       "    [['+', {'GroupNorm'}], ['+', {'GroupNorm'}], ['+', {'GroupNorm'}]]],\n",
       "   ['speechbrain/pretrained/interfaces.py', []],\n",
       "   ['tests/unittests/test_normalization.py',\n",
       "    [['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}]]]]],\n",
       " ['8da1396d30a7f0332f9c4d24de46d9a3a786885a',\n",
       "  [['recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py', []]]],\n",
       " ['adf5e11d2ccee3ce2be9556a2b87bc9c2a513e1c',\n",
       "  [['recipes/CommonLanguage/lang_id/train.py', []],\n",
       "   ['recipes/self-supervised-learning/wav2vec2/common_voice_prepare.py', []],\n",
       "   ['recipes/self-supervised-learning/wav2vec2/train.py', []],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/lobes/models/Xvector.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'Transformer'}], ['+', {'batch_size'}], ['+', {'batch_size'}]]],\n",
       "   ['speechbrain/nnet/CNN.py', []],\n",
       "   ['speechbrain/pretrained/interfaces.py', []]]],\n",
       " ['2173ac77cbfd4430980d6ed93fa618f0d499851f',\n",
       "  [['speechbrain/nnet/losses.py', []]]],\n",
       " ['e733e783b18c04bfc09ca2ff9dc7ba3531da002e',\n",
       "  [['recipes/CommonVoice/ASR/transducer/train.py', []]]],\n",
       " ['8543a57ed31e2a472f22f7b7c3739ffad1e1a854',\n",
       "  [['speechbrain/nnet/losses.py', []]]],\n",
       " ['ebda6783792e0615321229e9d60dfe4bcc881beb',\n",
       "  [['speechbrain/nnet/losses.py', []]]],\n",
       " ['f65ee157bd29dc102f2a6520572d0453770febc0',\n",
       "  [['recipes/LibriSpeech/ASR/transducer/train.py', []]]],\n",
       " ['8ee65aaf9a14ae64365b30aedf02e35a2fd8702b',\n",
       "  [['recipes/TIMIT/ASR/transducer/train.py', []],\n",
       "   ['recipes/TIMIT/ASR/transducer/train_wav2vec.py', []],\n",
       "   ['speechbrain/nnet/losses.py', []]]],\n",
       " ['68926f54ee711fae8599fee4c0a3c0ca9edd2cb1',\n",
       "  [['speechbrain/dataio/dataloader.py', []]]],\n",
       " ['38131a0db5ff90da9f34ad70c7c8e02141d6b453',\n",
       "  [['speechbrain/decoders/seq2seq.py', []]]],\n",
       " ['134688deeb60a47a320d91f59bc62bb3ef439ebd',\n",
       "  [['recipes/SLURP/direct/train_with_wav2vec2.py', []]]],\n",
       " ['3a41491a1ddca9734e3d5729eb181670b82e8b90',\n",
       "  [['speechbrain/nnet/normalization.py',\n",
       "    [['+', {'GroupNorm'}], ['+', {'GroupNorm'}], ['+', {'GroupNorm'}]]],\n",
       "   ['tests/unittests/test_normalization.py',\n",
       "    [['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}]]]]],\n",
       " ['31685795b44f5654af564b6432f253119b7b9561',\n",
       "  [['speechbrain/nnet/normalization.py', []]]],\n",
       " ['4d0eee384682f1a70f59ca099754fab875f4c04d',\n",
       "  [['tests/unittests/test_normalization.py', []]]],\n",
       " ['ce063709f5eb18e8811168e5f5a645be9f57df1c',\n",
       "  [['speechbrain/nnet/normalization.py',\n",
       "    [['+', {'GroupNorm'}], ['+', {'GroupNorm'}], ['+', {'GroupNorm'}]]],\n",
       "   ['tests/unittests/test_normalization.py',\n",
       "    [['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}],\n",
       "     ['+', {'GroupNorm'}]]]]],\n",
       " ['229c8219b37aee6a96a8a8d3b7fd119ea4815f93',\n",
       "  [['recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py', []]]],\n",
       " ['a4a906e2d6d679d2e9a1fe010e5eade43c2e9306',\n",
       "  [['recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py', []]]],\n",
       " ['1f448ce43de7bec06d192112ba0a80062bd79b34',\n",
       "  [['recipes/LibriSpeech/ASR/seq2seq/train.py', []]]],\n",
       " ['210a748cbc1da5e8d7d6066e13784c5563d7ab47',\n",
       "  [['recipes/LibriSpeech/ASR/seq2seq/train.py', []]]],\n",
       " ['59999b1db4a1057fcd7b1f496cecda6c58e37289',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/common_voice_prepare.py',\n",
       "    []],\n",
       "   ['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py',\n",
       "    [['+', {'Transformer'}]]],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'Transformer'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]]]],\n",
       " ['b085cf69eeec846d3ca4697bf60a23a4e632067b', []],\n",
       " ['3c5ed4185ed65c3d4a8b18ad96cc1192c0d2d0e0', []],\n",
       " ['e41fd07771d91845acd25435734fad58e68d1616', []],\n",
       " ['b9bd0d77ae4128fc147a8e4ba778ccd6427bb7d3', [['speechbrain/core.py', []]]],\n",
       " ['7c66413f68714ba6f97c1d7436d64fff3e443efc', [['speechbrain/core.py', []]]],\n",
       " ['a28bcf424535e99fcc923a8368437c2a4df3cd1a', []],\n",
       " ['4c7dc977cb77bbdc5dd05af762155a9dcdbf4afe', [['speechbrain/core.py', []]]],\n",
       " ['b330281ba09dca40cc0db41920ba7ef42070a824', [['speechbrain/core.py', []]]],\n",
       " ['b622744cbc45cbe2fdc2d08a3db7e3cda92948ed', []],\n",
       " ['e7c0073acc1830b191410419004a20fd6240f5b3', []],\n",
       " ['3974e4e2c55a8e0cbd267a3c151ee7d722af8096',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []]]],\n",
       " ['9543ab089b1844ed8ab1aef004d0c545f7db7bac',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []]]],\n",
       " ['5533cc2ce58a5f1990a6beb17ab61513931ac61b', []],\n",
       " ['fe8367aa19c3a91800a80424ebdfa2c8347aae97',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'batch_size'}], ['+', {'batch_size'}]]]]],\n",
       " ['d8b84cf7a01c5b43b2ed15002f7c92f3bd76994b', []],\n",
       " ['f0cab3643a40b62ad19d5a3eaafbc8d19f4805e4',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'batch_size'}]]]]],\n",
       " ['749d31d0b01407a71e5e1cf04421dbd7533d5c74', []],\n",
       " ['d1a3343465698dab73124a0c6ccc4a3fd2c16e7a', []],\n",
       " ['6ce00048b62bc8b0744b8de8a8a2cd25197de651', []],\n",
       " ['ac61d1f7d29343f8a217c6d90a4cfebce6a2d0cf', []],\n",
       " ['f0b5286853411c1e9e89e74cdad32ed51911aee5',\n",
       "  [['recipes/SLURP/direct/train_with_wav2vec2.py', []]]],\n",
       " ['33226905627d0cec01b3f18e97f713251650a9b4',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['ec86aabc317b5801ac9128352ed83d35f7d56a6c',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['005d62e017f56e1f11d281403ddf7885826549f7',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['d284b2d04f662478a4c0f93442a8c5bd52b7659f',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['39b9dd07cd0d0c9aad94d3b2f623441809c052e6',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['92148e8bc6bff627dc246c7413a7a69cfae2689a',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['8a2c07820baa979804b6575b242e6f9160222763',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['2054d547da734c2a3f0c6e6ef98fd4c62f675d3f',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['2c99be98f878e2174d9bc8c3c4bb287e624ced12',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['02f19dc4d78f453b912354eb2a17746c8cc4dc88',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['1c984fda9a6b5fa4fdde537792b9e6cfa107e72a',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['3b2428cfdee057512e8433d6de918039757bae8f',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['fad96acb8430167d754eb5f165b9a7720a50e47e',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['a4d22103bb3333d81fb11137d89a5a35057fbc8b',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['fc71dfc14d8e7b570ed4bf77c9b6ca3d2684d706',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['eff65547d74c17dbcb271f215c882cd0979603c6',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['237d3c3e087e8a16246277b3914e91da22bee94e',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['ca299b768550b2c7c45873090bf95faba05cb57c',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['63093209a46a606675f605e3bb931f7498d88995',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['6ce3df1a4b905f50505f3f0bae209c323a32fd5c',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['7d4a87152ea9a6804e8a69ea51cd165bbe482946', []],\n",
       " ['8a3b3eb7ce61c26119cf5ccdf90701af5d36faf7', []],\n",
       " ['5b14e8cae70c046a0b6ebd79deea342bc5fdcb35',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['9855e71d4022a720831c4a8da4ae677d8390fe34',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['6171307714ea67832501107a521dc856bbb8fd0e',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['1b1ea74e24888a11b47a8a7da71e891cec4f3c44',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['9bbebf1a3bbad6a5bbba21efd9853bac45b28cb4',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['c69f8b433425084feba5935578fb7a4e6729619c',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['c615dba310aaba5a23d8f19f4632a7a137aee20c',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['84654fc0e54862d1e88bdcac07c18ac71fafe923',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['62b8b85158deef426e82c6bbd8c6a220902f2c25',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['35c645e0f6bc3888901d6490b06102a6b281e1a5',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['fc820f5f0a1b420f33135d2c967db4284f7f1079',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['28a61e3f4a52caf6b0cf631c64829313814609a0',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['228828bdcc49d6f117c47d14df15c4b85bea69c5',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['6142bc3e025b532583476b00ed1d5996258e10fe',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['-', {'batch_size'}],\n",
       "     ['-', {'batch_size'}],\n",
       "     ['-', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]]]],\n",
       " ['a34aa84b4f54fecbeaec56cda4abf6e92d17c4e5',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []]]],\n",
       " ['e0709884573411eab9331d43d5453734a1c584c0',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []]]],\n",
       " ['7caa575d5902aee0b7ad432bb81ba62f96099b54',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []]]],\n",
       " ['f18225c60adff18cb0665ee8c5fc154c0e1f1896',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []]]],\n",
       " ['1b8f2dcbe6d89624d4751c06ca8f53657620a8c0',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['-', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['-', {'batch_size'}]]]]],\n",
       " ['3f73c2ea3c40d4c69623a00aa45896e7774b7e29',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['6b6c0d459420fc8f5449da1d9c0fa9e6291f0526',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['2913cd522e911737c4f16d42a08b3e8367fad5c7',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['-', {'batch_size'}], ['+', {'batch_size'}]]]]],\n",
       " ['927c02fbc96f6a9c3425c4d9431c0827deda3c3d',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['-', {'batch_size'}], ['+', {'batch_size'}]]]]],\n",
       " ['83de14e094eb1ae14d6fe3bca0c5f375b5059e18',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['+', {'batch_size'}]]]]],\n",
       " ['952c28c044b9f8349fe50a58786b8a9d95b84e88',\n",
       "  [['recipes/Voicebank/voicebank_prepare.py', []],\n",
       "   ['recipes/WHAMandWHAMR/enhancement/dynamic_mixing.py',\n",
       "    [['+', {'batch_size'}], ['+', {'batch_size'}], ['+', {'batch_size'}]]],\n",
       "   ['recipes/WHAMandWHAMR/enhancement/train.py', [['+', {'batch_size'}]]],\n",
       "   ['recipes/WHAMandWHAMR/prepare_data.py', []]]],\n",
       " ['09b3e81f093c89dc658fa99aedf7ce3706539774',\n",
       "  [['recipes/WHAMandWHAMR/enhancement/train.py', []]]],\n",
       " ['fd6ec51c5a868ea79c3707a8befe7275a249f9df',\n",
       "  [['recipes/WHAMandWHAMR/enhancement/train.py', []]]],\n",
       " ['74c1cbc4e0763dec81debf788ad27728457698e4',\n",
       "  [['recipes/WHAMandWHAMR/enhancement/train.py', []]]],\n",
       " ['f701a0ae61184ea431047a7aef167d7569dc930c', []],\n",
       " ['ea51f320ec8511aeb2f31ca3f8706348c7a509d9',\n",
       "  [['recipes/WHAMandWHAMR/enhancement/train.py', []]]],\n",
       " ['1067c14a1e260031af4276ac356ee2f965ceceb4', []],\n",
       " ['6e8f6af10efa0192956b1b2b8c3aeb92a2751bc5',\n",
       "  [['speechbrain/processing/features.py', []]]],\n",
       " ['6e50b207608faa7bb2158668b4589669f19ec852', []],\n",
       " ['84553490ee09db7ba047f3792b764f664616e9d5',\n",
       "  [['recipes/Voicebank/voicebank_prepare.py', []],\n",
       "   ['recipes/WHAMandWHAMR/enhancement/train.py', []]]],\n",
       " ['f24d5e67f3eabd257686154e6bd95bc41515fb06',\n",
       "  [['speechbrain/dataio/sampler.py', []]]],\n",
       " ['6938c9639af25fecf5ead08d8f39d8fd36995429',\n",
       "  [['speechbrain/dataio/sampler.py', []]]],\n",
       " ['c53fa4321817d60eb47bc3f83f2b1257597b0596',\n",
       "  [['speechbrain/dataio/sampler.py', [['-', {'batch_size'}]]]]],\n",
       " ['8bb1753d5921fc25690f30450828925f5d367776',\n",
       "  [['speechbrain/dataio/sampler.py', [['-', {'batch_size'}]]]]],\n",
       " ['9df49e17c018c9617dfd3161a7db50a75086603e',\n",
       "  [['recipes/IEMOCAP/emotion_recognition/train_with_wav2vec2.py', []],\n",
       "   ['recipes/SLURP/direct/train.py', []],\n",
       "   ['recipes/SLURP/direct/train_with_wav2vec2.py', []]]],\n",
       " ['22ce5c5fe6378c4f388c5f10ca0131a18cc6afae',\n",
       "  [['speechbrain/alignment/ctc_segmentation.py', []]]],\n",
       " ['9a11f0c9405e6ab68828ef6e69a9a4ec4044e7a0',\n",
       "  [['recipes/IEMOCAP/emotion_recognition/train_with_wav2vec2.py', []],\n",
       "   ['recipes/SLURP/direct/train_with_wav2vec2.py', []]]],\n",
       " ['6310e8624db822a01e3ccd98c1e489451e61190a', []],\n",
       " ['d5ea37e4f13a2420905f234122d7f40750c9728a', []],\n",
       " ['18e669b3d5fb938f1d2ffcd521b6de3891730cc5', []],\n",
       " ['c85ec434c5b6d0ebafdc374e10090ed85013244c', [['speechbrain/core.py', []]]],\n",
       " ['6bcebdb350e76d8541a8f0fa5897c66eb90ecf52', []],\n",
       " ['59e7ff10502a2c7f6cdb44f7258fa38dd8ba96e6',\n",
       "  [['recipes/SLURP/direct/train_with_wav2vec2.py', []]]],\n",
       " ['2bf7e6f5f00c9a18a50dac61acadcd5a19f87d10', []],\n",
       " ['e04721b808d2717a1277d7040f41ff284187ff5d',\n",
       "  [['recipes/IEMOCAP/emotion_recognition/train_with_wav2vec2.py', []]]],\n",
       " ['6937eec92644fa7146070f85cfed0b6bb50508de', []],\n",
       " ['88a26f8fb29bab41efcbfa9f6e116d91b010bc79', []],\n",
       " ['b9c66c2cbf0da086513e4728d534b46978873431', []],\n",
       " ['e00d0e8c8b4bafb7a2f1716d14c86848756b2313',\n",
       "  [['speechbrain/tokenizers/SentencePiece.py', []]]],\n",
       " ['2e1b727a3fa2c573298fb6435ddf6e7bc3f6c7bb',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['44862adee7034e9889eeb68b7f529be014b6f54d',\n",
       "  [['speechbrain/lobes/models/huggingface_wav2vec.py', []]]],\n",
       " ['33be00110d88eb1ff9184cb16ad9f7d0b88eaab9',\n",
       "  [['conftest.py', []],\n",
       "   ['recipes/Voicebank/dereverb/MetricGAN-U/train.py',\n",
       "    [['+', {'batch_size'}],\n",
       "     ['+', {'epochs'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]],\n",
       "   ['recipes/Voicebank/dereverb/MetricGAN-U/voicebank_revb_prepare.py', []],\n",
       "   ['recipes/Voicebank/dereverb/spectral_mask/train.py', []],\n",
       "   ['recipes/Voicebank/dereverb/spectral_mask/voicebank_revb_prepare.py', []],\n",
       "   ['recipes/Voicebank/enhance/MetricGAN-U/train.py',\n",
       "    [['+', {'batch_size'}],\n",
       "     ['+', {'epochs'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]],\n",
       "   ['recipes/Voicebank/enhance/MetricGAN-U/voicebank_prepare.py', []],\n",
       "   ['recipes/Voicebank/enhance/MetricGAN/train.py', []],\n",
       "   ['recipes/Voicebank/voicebank_prepare.py', []],\n",
       "   ['speechbrain/alignment/aligner.py',\n",
       "    [['-', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['-', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]],\n",
       "   ['speechbrain/core.py', []],\n",
       "   ['speechbrain/lobes/models/MetricGAN.py', []],\n",
       "   ['speechbrain/lobes/models/MetricGAN_U.py',\n",
       "    [['+', {'Linear'}],\n",
       "     ['+', {'LSTM'}],\n",
       "     ['+', {'LSTM'}],\n",
       "     ['+', {'LSTM'}],\n",
       "     ['+', {'LeakyReLU'}],\n",
       "     ['+', {'LSTM', 'RNN'}],\n",
       "     ['+', {'Sigmoid'}],\n",
       "     ['+', {'LeakyReLU'}],\n",
       "     ['+', {'BatchNorm2d'}],\n",
       "     ['+', {'Conv2d'}],\n",
       "     ['+', {'Conv2d'}],\n",
       "     ['+', {'Conv2d'}],\n",
       "     ['+', {'Conv2d'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['+', {'Linear'}]]],\n",
       "   ['speechbrain/nnet/complex_networks/c_RNN.py', []],\n",
       "   ['speechbrain/nnet/loss/si_snr_loss.py',\n",
       "    [['+', {'batch_size'}], ['+', {'batch_size'}], ['+', {'batch_size'}]]],\n",
       "   ['speechbrain/nnet/losses.py', []],\n",
       "   ['speechbrain/pretrained/fetching.py', []],\n",
       "   ['speechbrain/pretrained/interfaces.py', []],\n",
       "   ['speechbrain/tokenizers/SentencePiece.py', []],\n",
       "   ['speechbrain/utils/hpopt.py', [['+', {'epochs'}]]],\n",
       "   ['speechbrain/utils/metric_stats.py', []],\n",
       "   ['speechbrain/utils/superpowers.py', []],\n",
       "   ['templates/hyperparameter_optimization_speaker_id/custom_model.py', []],\n",
       "   ['templates/hyperparameter_optimization_speaker_id/mini_librispeech_prepare.py',\n",
       "    []],\n",
       "   ['templates/hyperparameter_optimization_speaker_id/train.py', []],\n",
       "   ['tests/integration/neural_networks/ASR_CTC/example_asr_ctc_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/ASR_CTC/example_asr_ctc_experiment_complex_net.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/ASR_CTC/example_asr_ctc_experiment_quaternion_net.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/ASR_DNN_HMM/example_asr_dnn_hmm_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/ASR_alignment_forward/example_asr_alignment_forward_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/ASR_alignment_viterbi/example_asr_alignment_viterbi_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/ASR_seq2seq/example_asr_seq2seq_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/G2P/example_g2p.py', []],\n",
       "   ['tests/integration/neural_networks/LM_RNN/example_lm_rnn_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/VAD/example_vad.py', []],\n",
       "   ['tests/integration/neural_networks/autoencoder/example_auto_experiment.py',\n",
       "    []],\n",
       "   ['tests/integration/neural_networks/enhance_GAN/example_enhance_gan_experiment.py',\n",
       "    [['-', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['-', {'batch_size'}],\n",
       "     ['-', {'batch_size'}],\n",
       "     ['+', {'batch_size'}],\n",
       "     ['+', {'batch_size'}]]],\n",
       "   ['tests/integration/neural_networks/separation/example_conv_tasnet.py', []],\n",
       "   ['tests/integration/neural_networks/speaker_id/example_xvector_experiment.py',\n",
       "    []],\n",
       "   ['tests/unittests/test_CNN.py',\n",
       "    [['-', {'Conv1d'}],\n",
       "     ['+', {'Conv1d'}],\n",
       "     ['-', {'Conv2d'}],\n",
       "     ['+', {'Conv2d'}]]],\n",
       "   ['tests/unittests/test_RNN.py',\n",
       "    [['-', {'RNN'}],\n",
       "     ['-', {'RNN'}],\n",
       "     ['-', {'RNN'}],\n",
       "     ['-', {'GRU'}],\n",
       "     ['-', {'LSTM'}],\n",
       "     ['-', {'GRU'}],\n",
       "     ['-', {'RNN'}],\n",
       "     ['-', {'RNN'}],\n",
       "     ['+', {'RNN'}],\n",
       "     ['+', {'GRU', 'LSTM', 'RNN'}],\n",
       "     ['-', {'RNN'}],\n",
       "     ['+', {'RNN'}]]],\n",
       "   ['tests/unittests/test_activations.py', []],\n",
       "   ['tests/unittests/test_attention.py', []],\n",
       "   ['tests/unittests/test_augment.py', []],\n",
       "   ['tests/unittests/test_batching.py', []],\n",
       "   ['tests/unittests/test_categorical_encoder.py', []],\n",
       "   ['tests/unittests/test_checkpoints.py',\n",
       "    [['-', {'Linear'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['-', {'Linear'}],\n",
       "     ['+', {'Linear'}]]],\n",
       "   ['tests/unittests/test_core.py', [['-', {'Linear'}], ['+', {'Linear'}]]],\n",
       "   ['tests/unittests/test_data_io.py', []],\n",
       "   ['tests/unittests/test_dataloader.py', []],\n",
       "   ['tests/unittests/test_dropout.py',\n",
       "    [['-', {'Dropout'}],\n",
       "     ['+', {'Dropout'}],\n",
       "     ['-', {'Dropout'}],\n",
       "     ['+', {'Dropout'}]]],\n",
       "   ['tests/unittests/test_embedding.py',\n",
       "    [['-', {'Embedding'}], ['+', {'Embedding'}]]],\n",
       "   ['tests/unittests/test_features.py', []],\n",
       "   ['tests/unittests/test_hpopt.py', []],\n",
       "   ['tests/unittests/test_linear.py', []],\n",
       "   ['tests/unittests/test_losses.py', []],\n",
       "   ['tests/unittests/test_metrics.py', []],\n",
       "   ['tests/unittests/test_multi_mic.py', []],\n",
       "   ['tests/unittests/test_normalization.py',\n",
       "    [['-', {'BatchNorm1d'}],\n",
       "     ['+', {'BatchNorm1d'}],\n",
       "     ['-', {'BatchNorm1d'}],\n",
       "     ['+', {'BatchNorm1d'}],\n",
       "     ['-', {'BatchNorm1d'}],\n",
       "     ['+', {'BatchNorm1d'}],\n",
       "     ['-', {'BatchNorm1d'}],\n",
       "     ['+', {'BatchNorm1d'}],\n",
       "     ['-', {'BatchNorm2d'}],\n",
       "     ['+', {'BatchNorm2d'}],\n",
       "     ['-', {'BatchNorm2d'}],\n",
       "     ['+', {'BatchNorm2d'}],\n",
       "     ['-', {'LayerNorm'}],\n",
       "     ['+', {'LayerNorm'}],\n",
       "     ['-', {'LayerNorm'}],\n",
       "     ['+', {'LayerNorm'}],\n",
       "     ['-', {'LayerNorm'}],\n",
       "     ['+', {'LayerNorm'}],\n",
       "     ['-', {'InstanceNorm1d'}],\n",
       "     ['+', {'InstanceNorm1d'}],\n",
       "     ['-', {'InstanceNorm1d'}],\n",
       "     ['+', {'InstanceNorm1d'}],\n",
       "     ['-', {'InstanceNorm2d'}],\n",
       "     ['+', {'InstanceNorm2d'}],\n",
       "     ['-', {'InstanceNorm2d'}],\n",
       "     ['+', {'InstanceNorm2d'}]]],\n",
       "   ['tests/unittests/test_pooling.py', []],\n",
       "   ['tests/unittests/test_pretrainer.py',\n",
       "    [['-', {'Linear'}],\n",
       "     ['+', {'Linear'}],\n",
       "     ['-', {'Linear'}],\n",
       "     ['+', {'Linear'}]]],\n",
       "   ['tests/unittests/test_samplers.py', []],\n",
       "   ['tests/unittests/test_signal_processing.py', []],\n",
       "   ['tests/unittests/test_tokenizer.py', []]]],\n",
       " ['faf56d60bcb6f4fd00c5fb8a6817c6b50047ad4f',\n",
       "  [['recipes/WHAMandWHAMR/enhancement/train.py', []]]],\n",
       " ['7207668ad1ea6705c66d30f987008299618af397', []],\n",
       " ['f8fc60748b0ab4b4c85841b1076463fa768089b5', []],\n",
       " ['c512adb7e0209949f13ca560efe5fbacbbc3f7b6',\n",
       "  [['recipes/WHAMandWHAMR/enhancement/dynamic_mixing.py',\n",
       "    [['+', {'batch_size'}], ['+', {'batch_size'}], ['+', {'batch_size'}]]],\n",
       "   ['recipes/WHAMandWHAMR/enhancement/train.py', [['+', {'batch_size'}]]],\n",
       "   ['recipes/WHAMandWHAMR/prepare_data.py', []]]],\n",
       " ['6c8a4ed8e66813a24eb949eeeaaee9bee168a76c',\n",
       "  [['recipes/CommonVoice/self-supervised-learning/wav2vec2/train.py', []],\n",
       "   ['speechbrain/lobes/models/huggingface_wav2vec.py',\n",
       "    [['-', {'batch_size'}]]]]],\n",
       " ['5424bb6b202f5f61ac370c66f4ee2a54061eedaf', [['speechbrain/core.py', []]]],\n",
       " ['66bbbbfe06bb854d4c1784619c90ac5923ac1def',\n",
       "  [['speechbrain/processing/features.py', []]]],\n",
       " ['a5ae53eb6dbe590ea0f31ecf8d67dac4ad543f20',\n",
       "  [['speechbrain/alignment/ctc_segmentation.py', []]]],\n",
       " ['87f205d65b4df68fcc6de4716dd78f8fb07a05ef', []],\n",
       " ['c20591f8004bf6b0f216f29ddfdd05cc2f22bb85',\n",
       "  [['speechbrain/alignment/ctc_segmentation.py', []]]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commit_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each commit has a list of files, each with summarised changes pertaining to any changes, especially Layer changes. We can combine this with commit information for comprehensive tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3122350a7f05c1b49a47014aee52ce7fbd2ff1c946247308f64993960c6bab2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
