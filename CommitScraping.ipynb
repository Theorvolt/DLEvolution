{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from tools import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(environ['user'],environ['token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Commits\n",
    "\n",
    "We can retrieve the patch/diff information of a commit. Note that this is one commit call, and Github is limited to 5000 requests an hour, i.e 5000 commits scraped. Let's sample some from SpeechBrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_list = list(map(lambda x:x['sha'],flatten([g.get_commits('speechbrain/speechbrain',{'page':x+1,'per_page':100}).json() for x in range(2)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.get_commit_diff('speechbrain/speechbrain',commit_list[0]).json()['files'][0]['patch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter words\n",
    "\n",
    "I have inputted some PyTorch layers to pick up in the process of reading line diffs. This is to be expanded for things like hyperparameters and eventually grouping by Layer/Hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_words = ['Conv1d','Conv2d','Conv3d', 'ConvTranspose1d','ConvTranspose1d','ConvTranspose2d','ConvTranspose3d','LazyConv1d','LazyConv2d','LazyConv3d','LazyConvTranspose1d','LazyConvTranspose2d','LazyConvTranspose3d','Unfold','Fold',\n",
    "'MaxPool1d', 'MaxPool2d', 'MaxPoool3d', 'MaxUnpool1d', 'MaxUnpool2d','MaxUnpool3d', 'AvgPool1d', 'AvgPool2d','AvgPool3d', 'LPPool1d','LPPool2d', 'LPPool3d', 'AdaptiveMaxPool1d', 'AdaptiveMaxPool2d', 'AdaptiveMaxPool3d', 'AdaptiveAvgPool1d',\n",
    "'AdaptiveAvgPool2d', 'AdaptiveAvgPool3d',\n",
    "'ReflectionPad1d', 'ReflectionPad2d', 'ReplicationPad1d', 'ReplicationPad2d', 'ReplicationPad3d', 'ZeroPad2d', 'ConstantPad1d', 'ConstantPad2d', 'ConstantPad3d',\n",
    "'ELU', 'Hardshrink', 'Hardsigmoid', 'Hardtanh', 'Hardswish', 'LeakyReLU', 'LogSigmoid', 'MultiheadAttention', 'PReLU', 'ReLU', 'RReLU', 'SELU', 'CELU', 'GELU', 'Sigmoid', 'SiLU', 'Mish', 'Softplus', 'Softshrink', 'Softsign', 'Tanh', 'Tanhshrink', 'Threshold',\n",
    "'Softmin', 'Softmax','Softmax2d', 'Softmax2d', 'LogSoftmax', 'AdaptiveLogSoftmaxWithLoss',\n",
    "'BatchNorm1d', 'BatchNorm2d', 'BatchNorm3d', 'GroupNorm', 'InstanceNorm1d', 'InstanceNorm2d', 'InstanceNorm3d', 'LayerNorm',\n",
    "'RNN', \"LSTM\", 'GRU', 'RNNCell', 'LSTMCell', 'GRUCell',\n",
    "'Transformer', 'TransformerEncoder', 'TransformerDecoder', 'Linear', 'Bilinear', 'LazyLinear', 'Identity',\n",
    "'Dropout', 'Dropout2d', 'Dropout3d', 'AlphaDropout',\n",
    "'Embedding',\n",
    "'batch_size', 'num_epochs', 'epochs', 'n_hidden'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commit_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for commit_sha in commit_list[:200]:\n",
    "    try:\n",
    "        commit = g.get_commit_diff('speechbrain/speechbrain',commit_sha).json()\n",
    "        com = commit['files']\n",
    "    except KeyError: \n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        commit_history.append([commit['sha'],[\n",
    "        [file['filename'],[\n",
    "        [line[0],set(re.findall('|'.join(filter_words), line))] for line in #  over each line, find all the unique instances of matches, along with +-\n",
    "        [line  for line in file['patch'].split('\\n') if  len(line) > 2 and ((line[0]=='+' and line[1] != '+') or (line[0]=='-' and line[1] != '-'))] # Split diff into list of lines\n",
    "        if len(set(re.findall('|'.join(filter_words), line))) != 0] # If there's no filtering, ignore\n",
    "        ] \n",
    "        for file in com if file['filename'].split('.')[-1] == 'py']]) # Filter python files\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can view the commit history with the list above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "commit_history"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
